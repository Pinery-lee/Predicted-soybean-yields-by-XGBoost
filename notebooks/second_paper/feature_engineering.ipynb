{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1b034-7119-47e0-9fe8-25eeb62a6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def read_data(path):\n",
    "    '''Read csv files, especially when the path has Chinese\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string. the path of csv file\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    data\n",
    "    '''\n",
    "    data = pd.read_csv(path,engine='python',encoding=\"utf8\")\n",
    "    return data\n",
    "\n",
    "def concatentate_data(input_dir,output_dir):\n",
    "    '''\n",
    "    It is a function which concatentates all csv file in a directory\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir: string. the input directory path of  csv files\n",
    "    output_dir: string. the output directory path of  csv files\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    None\n",
    "    '''\n",
    "    if os.listdir(outputdir) is not None:\n",
    "        print('have been concated before! Do not repeat!')\n",
    "        return\n",
    "    data_list = []\n",
    "    for _, _, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    data = read_data(os.path.join(input_dir,file))\n",
    "                    data['Year'] = int(file[-8:-4])\n",
    "                    data_list.append(data)\n",
    "            combined_csv = pd.concat(data_list)\n",
    "    # Save new csv file of yield\n",
    "    combined_csv.to_csv(os.path.join(output_dir, 'features.csv'), index=False)\n",
    "    print(\"Processed!\")             \n",
    "                \n",
    "    return None\n",
    "\n",
    "def merge_data(data1,data2):\n",
    "    '''\n",
    "    It is a function which merges two data according the sta_con and year columns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data1: df\n",
    "    data2: df\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    data: been merged\n",
    "    '''\n",
    "    data = data1.merge(data2,on=['Year','sta_con'],how='left',validate='many_to_one')\n",
    "    return data\n",
    "    \n",
    "def ph_time_toint(phenological_data):\n",
    "    '''将25%、50%、75%的物候时间点变成序列整数\n",
    "       phenological_data：物候表格数据\n",
    "       '''\n",
    "    ph12 = phenological_data\n",
    "    for i in [3,4,5]:\n",
    "        for j in range(len(ph12.index.to_list())):\n",
    "            year = str(ph12.iloc[j,i].year)\n",
    "            \n",
    "            if pd.to_datetime(year+'-04-10') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-04-01'):\n",
    "                ph12.iloc[j,i] = 9\n",
    "            elif pd.to_datetime(year+'-04-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-04-10'):\n",
    "                ph12.iloc[j,i] = 10\n",
    "            elif pd.to_datetime(year+'-05-01') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-04-20'):\n",
    "                ph12.iloc[j,i] = 11\n",
    "\n",
    "            elif pd.to_datetime(year+'-05-10') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-05-01'):\n",
    "                ph12.iloc[j,i] = 12\n",
    "            elif pd.to_datetime(year+'-05-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-05-10'):\n",
    "                ph12.iloc[j,i] = 13\n",
    "            elif pd.to_datetime(year+'-06-01') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-05-20'):\n",
    "                ph12.iloc[j,i] = 14\n",
    "\n",
    "            elif pd.to_datetime(year+'-06-10') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-06-01'):\n",
    "                ph12.iloc[j,i] = 15\n",
    "            elif pd.to_datetime(year+'-06-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-06-10'):\n",
    "                ph12.iloc[j,i] = 16\n",
    "            elif pd.to_datetime(year+'-07-01') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-06-20'):\n",
    "                ph12.iloc[j,i] = 17\n",
    "\n",
    "            elif pd.to_datetime(year+'-07-10') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-07-01'):\n",
    "                ph12.iloc[j,i] = 18\n",
    "            elif pd.to_datetime(year+'-07-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-07-10'):\n",
    "                ph12.iloc[j,i] = 19\n",
    "            elif pd.to_datetime(year+'-08-01') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-07-20'):\n",
    "                ph12.iloc[j,i] = 20\n",
    "\n",
    "            elif pd.to_datetime(year+'-08-10') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-08-01'):\n",
    "                ph12.iloc[j,i] = 21\n",
    "            elif pd.to_datetime(year+'-08-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-08-10'):\n",
    "                ph12.iloc[j,i] = 22\n",
    "            elif pd.to_datetime(year+'-09-01') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-08-20'):\n",
    "                ph12.iloc[j,i] = 23\n",
    "\n",
    "            elif pd.to_datetime(year+'-09-10') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-09-01'):\n",
    "                ph12.iloc[j,i] = 24\n",
    "            elif pd.to_datetime(year+'-09-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-09-10'):\n",
    "                ph12.iloc[j,i] = 25\n",
    "            elif pd.to_datetime(year+'-10-01') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-09-20'):\n",
    "                ph12.iloc[j,i] = 26\n",
    "\n",
    "            elif pd.to_datetime(year+'-10-10') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-10-01'):\n",
    "                ph12.iloc[j,i] = 27\n",
    "            elif pd.to_datetime(year+'-10-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-10-10'):\n",
    "                ph12.iloc[j,i] = 28\n",
    "            elif pd.to_datetime(year+'-11-01') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-10-20'):\n",
    "                ph12.iloc[j,i] = 29\n",
    "\n",
    "            elif pd.to_datetime(year+'-11-10') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-11-01'):\n",
    "                ph12.iloc[j,i] = 30\n",
    "            elif pd.to_datetime(year+'-11-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-11-10'):\n",
    "                ph12.iloc[j,i] = 31\n",
    "            elif pd.to_datetime(year+'-12-01') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-11-20'):\n",
    "                ph12.iloc[j,i] = 32\n",
    "            elif pd.to_datetime(year+'-12-20') > pd.to_datetime(ph12.iloc[j,i]) >= pd.to_datetime(year+'-12-01'):\n",
    "                ph12.iloc[j,i] = 33\n",
    "    return  ph12\n",
    "\n",
    "def add_mean_features(phenology_alldata,yield_data,feature_list,stage = 'Planted',crop = '大豆'):\n",
    "    '''计算作物某一物候阶段的某些特征的平均值\n",
    "       phenology_alldata：物候数据\n",
    "       yield_data：包含特征\n",
    "       stage：string.作物物候\n",
    "       feature_list:list,要平均的特征列表\n",
    "       '''\n",
    "    phenology_stage = phenology_alldata[phenology_alldata[crop+'物候'] == stage]# 筛选出种植期\n",
    "    phenology_stage = ph_time_toint(phenology_stage)# 时间点变成整数序列\n",
    "    for i in range(phenology_stage.shape[0]):\n",
    "        phenology_stage.iloc[i,1] = phenology_stage.iloc[i,1].lower()\n",
    "    phenology_stage.rename(columns={'年份': 'Year', '州': 'State',crop+'物候':'Phenology'},inplace=True)# 重命名为英文并与产量数据保持一致\n",
    "    try_data = yield_data.merge(phenology_stage,on=['Year','State'],how='left',validate='many_to_one')# 融合在一起\n",
    "    # print(try_data)\n",
    "    for feature in feature_list:\n",
    "        column_name = stage+'_'+feature# 确定列名\n",
    "        try_data[column_name] = np.nan# 创建新列\n",
    "        # phenology_alldata.to_excel('D:/毕业论文/物候数据/大豆物候表已处理/物候总表.xlsx')\n",
    "        for row in range(try_data.shape[0]):\n",
    "            cal_columns = []\n",
    "            if try_data.loc[row,25] is not np.nan:\n",
    "                # print(try_data.loc[row,25],try_data.loc[row,75],type(try_data.loc[row,75]))\n",
    "                for i in list(range(try_data.loc[row,25],try_data.loc[row,75]+1)):\n",
    "                    cal_columns.append(str(i)+'_'+feature)# 确定哪几列参与计算\n",
    "                try_data.loc[row,column_name] = try_data.loc[row,cal_columns].mean()# 求平均\n",
    "    try_data = try_data.drop(columns=[ 'Phenology',25,50,75])\n",
    "    return try_data\n",
    "\n",
    "def add_sum_features(phenology_alldata,yield_data,feature_list,stage = 'Planted',crop = '大豆'):\n",
    "    '''计算作物某一物候阶段的某些特征的累计求和值\n",
    "       phenology_alldata：物候数据\n",
    "       yield_data：包含特征\n",
    "       stage：string.作物物候\n",
    "       feature_list:list,要求和的特征列表\n",
    "       '''\n",
    "    phenology_stage = phenology_alldata[phenology_alldata[crop+'物候'] == stage]# 筛选出种植期\n",
    "    phenology_stage = ph_time_toint(phenology_stage)# 时间点变成整数序列\n",
    "    for i in range(phenology_stage.shape[0]):\n",
    "        phenology_stage.iloc[i,1] = phenology_stage.iloc[i,1].lower()\n",
    "    phenology_stage.rename(columns={'年份': 'Year', '州': 'State',crop+'物候':'Phenology'},inplace=True)# 重命名为英文并与产量数据保持一致\n",
    "    try_data = yield_data.merge(phenology_stage,on=['Year','State'],how='left',validate='many_to_one')# 融合在一起\n",
    "    # print(try_data)\n",
    "    for feature in feature_list:\n",
    "        column_name = stage+'_'+feature# 确定列名\n",
    "        try_data[column_name] = np.nan# 创建新列\n",
    "        # phenology_alldata.to_excel('D:/毕业论文/物候数据/大豆物候表已处理/物候总表.xlsx')\n",
    "        for row in range(try_data.shape[0]):\n",
    "            cal_columns = []\n",
    "            if try_data.loc[row,25] is not np.nan:\n",
    "                # print(try_data.loc[row,25],try_data.loc[row,75],type(try_data.loc[row,75]))\n",
    "                for i in list(range(try_data.loc[row,25],try_data.loc[row,75]+1)):\n",
    "                    cal_columns.append(str(i)+'_'+feature)# 确定哪几列参与计算\n",
    "                try_data.loc[row,column_name] = try_data.loc[row,cal_columns].sum()# 求和\n",
    "    try_data = try_data.drop(columns=[ 'Phenology',25,50,75])\n",
    "    return try_data\n",
    "\n",
    "def dist(lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two points based on latitude and longitude.\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lat1, long1, lat2, long2 = map(radians, [lat1, long1, lat2, long2])\n",
    "    # haversine formula \n",
    "    dlon = long2 - long1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km\n",
    "\n",
    "def find_nearest(county,lat, long, N):\n",
    "    \"\"\"\n",
    "    Calculate the N counties with the smallest distance to a specific county\n",
    "    \"\"\"\n",
    "    distances = county.apply(\n",
    "        lambda row: dist(lat, long, row['Latitude'], row['Longitude']), \n",
    "        axis=1)\n",
    "    return list(county.loc[distances.nsmallest(n=N,keep = 'all').index, 'sta_con'])\n",
    "\n",
    "\n",
    "def average_yield_of_N_years(data,N):\n",
    "    '''It's a function that calculates the average yield for the previous N years of a given year, of each county.\n",
    "    \n",
    "    Paramaters\n",
    "    ----------\n",
    "    data:DF, Yield data\n",
    "    N:int,Average yield of the past N years involved in the calculation\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    data\n",
    "    '''\n",
    "    print('**********average yield**********')\n",
    "    for county in set(data[data['Year'].isin([i for i in range(2004,2022)])].sta_con):\n",
    "        for year in range(2004,2022):\n",
    "            yearlist = [i for i in range(year-N,year)]\n",
    "            data.loc[(data['Year']==year) & (data['sta_con']==county),'yield_N'+str(N)+'_EXP2'] = data[(data['Year'].isin(yearlist)) & (data['sta_con']==county)]['yield(t/ha)'].mean()\n",
    "    data = data.fillna(data.groupby(['Year','State']).transform('mean'))\n",
    "    return data\n",
    "\n",
    "def linear_yield_of_N_years(data,N):\n",
    "    '''It's a function that calculates the linear yield for the previous N years of a given year, of each county.\n",
    "    \n",
    "    Paramaters\n",
    "    ----------\n",
    "    data:DF, Yield data\n",
    "    N:int,linear yield of the past N years involved in the calculation\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    data\n",
    "    '''\n",
    "    print('**********linear yield**********')\n",
    "    for county in set(data[data['Year'].isin([i for i in range(2004,2022)])].sta_con):\n",
    "        for year in range(2004,2022):\n",
    "            yearlist = [i for i in range(year-N,year)]\n",
    "            print(county,year)\n",
    "            data1 = data[(data['Year'].isin(yearlist)) & (data['sta_con']==county)][['Year','yield(t/ha)']]\n",
    "            print(data1)\n",
    "            if len(data1) == 1:\n",
    "                data.loc[(data['Year']==year) & (data['sta_con']==county),'yield_N'+str(N)+'_EXP3'] = data.loc[(data['Year'].isin(yearlist)) & (data['sta_con']==county),'yield(t/ha)'].mean()\n",
    "            elif len(data1) >1:\n",
    "                linear_m = linear_model.LinearRegression()\n",
    "                linear_m.fit(np.array(data1['Year']).reshape(-1,1),np.array(data1['yield(t/ha)']).reshape(-1,1))\n",
    "                slope = linear_m.coef_[0]\n",
    "                intercept = linear_m.intercept_\n",
    "                print(slope,intercept)\n",
    "                data.loc[(data['Year']==year) & (data['sta_con']==county),'yield_N'+str(N)+'_EXP3'] = slope*data.loc[(data['Year']==year) & (data['sta_con']==county),'Year']+intercept\n",
    "        data = data.fillna(data.groupby(['Year','State']).transform('mean'))\n",
    "    return data\n",
    "\n",
    "def average_yield_of_N_years_nearest(data,N_year = 30,N_nearest = 6):\n",
    "    '''It's a function that calculates the average yield for the previous N_year years from N_nearest counties of a given year, of each county.\n",
    "    \n",
    "    Paramaters\n",
    "    ----------\n",
    "    data:DF, Yield data\n",
    "    N_year:int,linear yield of the past N years involved in the calculation\n",
    "    N_nearest:int,linear yield of the nearest N counties involved in the calculation\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    data\n",
    "    '''\n",
    "    print('**********average nearest yield**********')\n",
    "    data['name'] = data.apply(\n",
    "    lambda row: find_nearest(data.drop_duplicates(subset=['sta_con','Longitude','Latitude']),row['Latitude'], row['Longitude'],N = N_nearest), \n",
    "    axis=1)\n",
    "    \n",
    "    for county in set(data[data['Year'].isin([i for i in range(2004,2022)])].sta_con):\n",
    "        for year in range(2004,2022):\n",
    "            yearlist = [i for i in range(year-N_year,year)]\n",
    "            countylist = data.loc[data['sta_con'] == county, 'name'].iloc[0]\n",
    "            print(county,year,countylist)\n",
    "            data1 = data[(data['Year'].isin(yearlist)) & (data['sta_con'].isin(countylist))][['Year','yield(t/ha)']]\n",
    "            print(data1)\n",
    "            data.loc[(data['Year']==year) & (data['sta_con']==county),'yield_N'+str(N_year)+'_EXP20'] = data1['yield(t/ha)'].mean()\n",
    "        data = data.fillna(data.groupby(['Year','State']).transform('mean'))\n",
    "    return data\n",
    "\n",
    "\n",
    "def linear_yield_of_N_years_nearest(data,N_year = 30,N_nearest = 6):\n",
    "    '''It's a function that calculates the linear yield for the previous N_year years of a given year, of each county from N_nearest counties.\n",
    "    \n",
    "    Paramaters\n",
    "    ----------\n",
    "    data:DF, Yield data\n",
    "    N_year:int,linear yield of the past N years involved in the calculation\n",
    "    N_nearest:int,linear yield of the nearest N counties involved in the calculation\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    data\n",
    "    '''\n",
    "    print('**********linear nearest yield**********')\n",
    "    data['name'] = data.apply(\n",
    "    lambda row: find_nearest(data.drop_duplicates(subset=['sta_con','Longitude','Latitude']),row['Latitude'], row['Longitude'],N = N_nearest), \n",
    "    axis=1)\n",
    "    \n",
    "    for county in set(data[data['Year'].isin([i for i in range(2004,2022)])].sta_con):\n",
    "        for year in range(2004,2022):\n",
    "            yearlist = [i for i in range(year-N_year,year)]\n",
    "            countylist = data.loc[data['sta_con'] == county, 'name'].iloc[0]\n",
    "            print(county,year,countylist)\n",
    "            data1 = data[(data['Year'].isin(yearlist)) & (data['sta_con'].isin(countylist))][['Year','yield(t/ha)']]\n",
    "            print(data1)\n",
    "            if len(data1) == 1:\n",
    "                data.loc[(data['Year']==year) & (data['sta_con']==county),'yield_N'+str(N_year)+'_EXP30'] = data.loc[(data['Year'].isin(yearlist)) & (data['sta_con']==county),'yield(t/ha)'].mean()\n",
    "            elif len(data1) >1:\n",
    "                linear_m = linear_model.LinearRegression()\n",
    "                linear_m.fit(np.array(data1['Year']).reshape(-1,1),np.array(data1['yield(t/ha)']).reshape(-1,1))\n",
    "                slope = linear_m.coef_[0]\n",
    "                intercept = linear_m.intercept_\n",
    "                print(slope,intercept)\n",
    "                data.loc[(data['Year']==year) & (data['sta_con']==county),'yield_N'+str(N_year)+'_EXP30'] = slope*data.loc[(data['Year']==year) & (data['sta_con']==county),'Year']+intercept\n",
    "        data = data.fillna(data.groupby(['Year','State']).transform('mean'))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62eaf13-5090-4ab4-8768-caae58dbb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成大豆物候特征\n",
    "# concatentate features from 2003 to 2021\n",
    "inputdir = 'D:/论文-产量趋势利用/数据/动态特征/raw/soybean'\n",
    "outputdir = 'D:/论文-产量趋势利用/数据/动态特征/processed/soybean'\n",
    "concatentate_data(inputdir,outputdir)\n",
    "\n",
    "# create phenology-based features\n",
    "phenology_data  = pd.read_excel('D:/论文-产量趋势利用/数据/物候数据/processed/soybean/2003-2021物候总表.xlsx')\n",
    "yield_data = read_data('D:/论文-产量趋势利用/数据/产量数据/processed/soybean/soybean.csv')\n",
    "feature_list = ['NDVI','EVI','LSWI','GCVI','RVI','SAVI','WDRVI','Fpar','Lai','LE','LST_Day_1km','LST_Night_1km',\n",
    "                'spi14d','spi30d','spi90d','eddi14d','eddi30d','eddi90d','spei14d','spei30d','spei90d','pdsi','z',\n",
    "                'sur_refl_b01','sur_refl_b02','sur_refl_b03','sur_refl_b04','sur_refl_b05','sur_refl_b06','sur_refl_b07']\n",
    "dy_features = read_data('D:/论文-产量趋势利用/数据/动态特征/processed/soybean/features.csv')\n",
    "data = merge_data(yield_data, dy_features)\n",
    "# mean values during the phenological stage\n",
    "# Planted Emergrd(后续修改) Blooming Setting Pods Dropping Leaves Harvested\n",
    "alldata = add_mean_features(phenology_data,data,feature_list,stage = 'Planted',crop = '大豆')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Emergrd',crop = '大豆')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Blooming',crop = '大豆')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Setting Pods',crop = '大豆')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Dropping Leaves',crop = '大豆')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Harvested',crop = '大豆')\n",
    "# sum values during the phenological stage\n",
    "feature_list = ['temperature','shortwave_radiation','longwave_radiation','ET']\n",
    "for ph in ['Planted','Emergrd','Blooming','Setting Pods','Dropping Leaves','Harvested']:\n",
    "    alldata = add_sum_features(phenology_data,alldata,feature_list,stage = ph, crop = '大豆')\n",
    "alldata.to_csv( 'D:/论文-产量趋势利用/数据/alldata_soybean.csv', index=False)\n",
    "alldata.loc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0caa8-1f96-4d96-ac88-55b9334cf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大豆数据瘦身得到输入数据\n",
    "inputdata = read_data('D:/论文-产量趋势利用/数据/alldata_soybean.csv')\n",
    "pic = read_data('D:/论文-产量趋势利用/数据/面积数据/soybean_PIC_resampled.csv')\n",
    "inputdata = merge_data(inputdata, pic)\n",
    "# 去除列\n",
    "drop_fea_list = ['Value','County','irrigated soybean area','soybean area']\n",
    "for i in ['LE','30d','shortwave','90d','RVI','Lai','Fpar','WDRVI']:\n",
    "    drop_fea_list += list(inputdata.filter(regex=i,axis = 1))\n",
    "inputdata = inputdata[inputdata.columns.drop(drop_fea_list)]\n",
    "print(inputdata.index)\n",
    "# 用一个州的平均灌溉水平插值PIC\n",
    "inputdata.PIC = inputdata.PIC.fillna(inputdata.groupby('State').transform('mean').PIC)\n",
    "\n",
    "# 去除行\n",
    "state_list = ['illinois','indiana','iowa','michigan','minnesota','missouri','nebraska','north dakota','ohio','south dakota','wisconsin']\n",
    "pattern = '|'.join(state_list)\n",
    "inputdata = inputdata[inputdata['sta_con'].str.contains(pattern)]\n",
    "\n",
    "# 去除大部分缺失行\n",
    "inputdata = inputdata.loc[inputdata['7_NDVI'].dropna().index,:]\n",
    "# 再填充\n",
    "inputdata = inputdata.fillna(inputdata.groupby('State').transform('mean'))\n",
    "inputdata.to_csv( 'D:/论文-产量趋势利用/数据/input_soybean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96323c50-bef2-4c2f-a8fa-c12846a4e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加经纬度列并计算试验列\n",
    "inputdata = read_data('D:/论文-产量趋势利用/数据/input_soybean.csv')\n",
    "lon_lat = read_data('D:/论文-产量趋势利用/数据/区划数据/processed/lon_latitude.csv')\n",
    "# read yield data from 1980 to now\n",
    "data1980 = pd.read_csv(r'D:/论文-产量趋势利用/数据/产量数据/processed/soybean/1980年以来的大豆单产.csv',engine = 'python')\n",
    "data1980 = data1980[data1980['Year'].isin([i for i in range(1980,2004,1)])]\n",
    "# Make the names of states and counties lowercase.\n",
    "data1980.loc[:,\"State\"] = data1980.loc[:,\"State\"].str.lower()\n",
    "data1980.loc[:,\"County\"] = data1980.loc[:,\"County\"].str.lower()\n",
    "\n",
    "# Concatenate state name and county name as sta_con.\n",
    "data1980.loc[:,\"sta_con\"] = data1980.loc[:,\"State\"] + \"_\" + data1980.loc[:,\"County\"]\n",
    "data1 = pd.concat([inputdata,data1980])\n",
    "data1 = data1.drop(columns = ['County'])\n",
    "data1 = data1.merge(lon_lat,on=['sta_con'],how='inner',validate='many_to_one')\n",
    "\n",
    "# print(len(set(inputdata.sta_con)),len(set(data1980.sta_con)),len(set(data1.sta_con)))\n",
    "# get the yield_N5_EXP2 column for 5 years平均产量\n",
    "data1 = average_yield_of_N_years(data1,5)\n",
    "# get the yield_N5_EXP3 column for 5 years线性产量（30年）\n",
    "data1 = linear_yield_of_N_years(data1,30)\n",
    "# get the yield_N5_EXP20 column for 30 years加上周围5个县的平均产量\n",
    "data1 = average_yield_of_N_years_nearest(data1,N_year = 5,N_nearest = 6)\n",
    "# get the yield_N5_EXP30 column for 30 years加上周围5个县的线性产量（30年）\n",
    "data1 = linear_yield_of_N_years_nearest(data1,N_year = 30,N_nearest = 6)\n",
    "# data1 = data1[data1['Year'].isin([i for i in range(2004,2022,1)])]\n",
    "data1.to_csv('D:/论文-产量趋势利用/数据/input_soybean_exp.csv', index=False)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9ae30-9d83-41d9-93d5-9587450e45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average 的年份测试\n",
    "# 添加经纬度列并计算试验列\n",
    "inputdata = read_data('D:/论文-产量趋势利用/数据/input_soybean.csv')\n",
    "lon_lat = read_data('D:/论文-产量趋势利用/数据/区划数据/processed/lon_latitude.csv')\n",
    "# read yield data from 1980 to now\n",
    "data1980 = pd.read_csv(r'D:/论文-产量趋势利用/数据/产量数据/processed/soybean/1980年以来的大豆单产.csv',engine = 'python')\n",
    "data1980 = data1980[data1980['Year'].isin([i for i in range(1980,2004,1)])]\n",
    "# Make the names of states and counties lowercase.\n",
    "data1980.loc[:,\"State\"] = data1980.loc[:,\"State\"].str.lower()\n",
    "data1980.loc[:,\"County\"] = data1980.loc[:,\"County\"].str.lower()\n",
    "\n",
    "# Concatenate state name and county name as sta_con.\n",
    "data1980.loc[:,\"sta_con\"] = data1980.loc[:,\"State\"] + \"_\" + data1980.loc[:,\"County\"]\n",
    "data1 = pd.concat([inputdata,data1980])\n",
    "data1 = data1.drop(columns = ['County'])\n",
    "data1 = data1.merge(lon_lat,on=['sta_con'],how='inner',validate='many_to_one')\n",
    "\n",
    "# print(len(set(inputdata.sta_con)),len(set(data1980.sta_con)),len(set(data1.sta_con)))\n",
    "# get the yield_N5_EXP2 column for 5 years平均产量\n",
    "for i in range(2,35):\n",
    "    data1 = average_yield_of_N_years(data1,i)\n",
    "\n",
    "# data1 = data1[data1['Year'].isin([i for i in range(2004,2022,1)])]\n",
    "data1.to_csv('D:/论文-产量趋势利用/数据/input_soybean_exp_ave.csv', index=False)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a5ee7-ed75-4640-8cd4-c0bfb1c1c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear 的年份测试\n",
    "# 添加经纬度列并计算试验列\n",
    "inputdata = read_data('D:/论文-产量趋势利用/数据/input_soybean.csv')\n",
    "lon_lat = read_data('D:/论文-产量趋势利用/数据/区划数据/processed/lon_latitude.csv')\n",
    "# read yield data from 1980 to now\n",
    "data1980 = pd.read_csv(r'D:/论文-产量趋势利用/数据/产量数据/processed/soybean/1980年以来的大豆单产.csv',engine = 'python')\n",
    "data1980 = data1980[data1980['Year'].isin([i for i in range(1980,2004,1)])]\n",
    "# Make the names of states and counties lowercase.\n",
    "data1980.loc[:,\"State\"] = data1980.loc[:,\"State\"].str.lower()\n",
    "data1980.loc[:,\"County\"] = data1980.loc[:,\"County\"].str.lower()\n",
    "\n",
    "# Concatenate state name and county name as sta_con.\n",
    "data1980.loc[:,\"sta_con\"] = data1980.loc[:,\"State\"] + \"_\" + data1980.loc[:,\"County\"]\n",
    "data1 = pd.concat([inputdata,data1980])\n",
    "data1 = data1.drop(columns = ['County'])\n",
    "data1 = data1.merge(lon_lat,on=['sta_con'],how='inner',validate='many_to_one')\n",
    "\n",
    "# print(len(set(inputdata.sta_con)),len(set(data1980.sta_con)),len(set(data1.sta_con)))\n",
    "# get the yield_N5_EXP2 column for 5 years平均产量\n",
    "for i in [3,5,10,15,20,25,30,35]:\n",
    "    data1 = linear_yield_of_N_years(data1,i)\n",
    "\n",
    "# data1 = data1[data1['Year'].isin([i for i in range(2004,2022,1)])]\n",
    "data1.to_csv('D:/论文-产量趋势利用/数据/input_soybean_exp_lin.csv', index=False)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a58320-ea74-4827-a2bf-65a9dbc4fb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975a9db-99de-4a70-8aa0-dc74c55ec1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成玉米物候特征\n",
    "# concatentate features from 2004 to 2021\n",
    "inputdir = 'D:/论文-产量趋势利用/数据/动态特征/raw/maize'\n",
    "outputdir = 'D:/论文-产量趋势利用/数据/动态特征/processed/maize'\n",
    "concatentate_data(inputdir,outputdir)\n",
    "\n",
    "# create phenology-based features\n",
    "phenology_data  = pd.read_excel('D:/论文-产量趋势利用/数据/物候数据/processed/maize/2003-2021物候总表.xlsx')\n",
    "yield_data = read_data('D:/论文-产量趋势利用/数据/产量数据/processed/maize/maize.csv')\n",
    "feature_list = ['NDVI','EVI','LSWI','GCVI','RVI','SAVI','WDRVI','Fpar','Lai','LE','LST_Day_1km','LST_Night_1km',\n",
    "                'spi14d','spi30d','spi90d','eddi14d','eddi30d','eddi90d','spei14d','spei30d','spei90d','pdsi','z',\n",
    "                'sur_refl_b01','sur_refl_b02','sur_refl_b03','sur_refl_b04','sur_refl_b05','sur_refl_b06','sur_refl_b07']\n",
    "dy_features = read_data('D:/论文-产量趋势利用/数据/动态特征/processed/maize/features.csv')\n",
    "data = merge_data(yield_data, dy_features)\n",
    "# mean values during the phenological stage\n",
    "# Planted Emergrd(后续修改) Blooming Setting Pods Dropping Leaves Harvested\n",
    "alldata = add_mean_features(phenology_data,data,feature_list,stage = 'Planted',crop = '玉米')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Emergrd',crop = '玉米')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Silking',crop = '玉米')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Dough',crop = '玉米')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Dented',crop = '玉米')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Mature',crop = '玉米')\n",
    "alldata = add_mean_features(phenology_data,alldata,feature_list,stage = 'Harvested',crop = '玉米')\n",
    "# sum values during the phenological stage\n",
    "feature_list = ['temperature','shortwave_radiation','longwave_radiation','ET']\n",
    "for ph in ['Planted','Emergrd','Silking','Dough','Dented','Mature','Harvested']:\n",
    "    alldata = add_sum_features(phenology_data,alldata,feature_list,stage = ph, crop = '玉米')\n",
    "alldata.to_csv( 'D:/论文-产量趋势利用/数据/alldata_maize.csv', index=False)\n",
    "alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d3845-603e-4a97-9843-00e7551df0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 玉米数据瘦身得到输入数据\n",
    "inputdata = read_data('D:/论文-产量趋势利用/数据/alldata_maize.csv')\n",
    "pic = read_data('D:/论文-产量趋势利用/数据/面积数据/maize_PIC_resampled.csv')\n",
    "inputdata = merge_data(inputdata, pic)\n",
    "# 去除列\n",
    "drop_fea_list = ['Value','County','irrigated maize area','maize area']\n",
    "for i in ['LE','30d','shortwave','90d','RVI','Lai','Fpar','WDRVI']:\n",
    "    drop_fea_list += list(inputdata.filter(regex=i,axis = 1))\n",
    "inputdata = inputdata[inputdata.columns.drop(drop_fea_list)]\n",
    "print(inputdata.index)\n",
    "# 用一个州的平均灌溉水平插值PIC\n",
    "inputdata.PIC = inputdata.PIC.fillna(inputdata.groupby('State').transform('mean').PIC)\n",
    "\n",
    "# 去除行\n",
    "state_list = ['illinois','indiana','iowa','michigan','minnesota','missouri','nebraska','north dakota','ohio','south dakota','wisconsin']\n",
    "pattern = '|'.join(state_list)\n",
    "inputdata = inputdata[inputdata['sta_con'].str.contains(pattern)]\n",
    "\n",
    "# 去除大部分缺失行\n",
    "inputdata = inputdata.loc[inputdata['7_NDVI'].dropna().index,:]\n",
    "# 再填充\n",
    "inputdata = inputdata.fillna(inputdata.groupby('State').transform('mean'))\n",
    "inputdata.to_csv( 'D:/论文-产量趋势利用/数据/input_maize.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f4427-5c9b-4f1f-9450-60a3726b007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加经纬度列并计算试验列\n",
    "inputdata = read_data('D:/论文-产量趋势利用/数据/input_maize.csv')\n",
    "lon_lat = read_data('D:/论文-产量趋势利用/数据/区划数据/processed/lon_latitude.csv')\n",
    "# read yield data from 1980 to now\n",
    "data1980 = pd.read_csv(r'D:/论文-产量趋势利用/数据/产量数据/processed/maize/1980年以来的玉米单产.csv',engine = 'python')\n",
    "data1980 = data1980[data1980['Year'].isin([i for i in range(1980,2004,1)])]\n",
    "# Make the names of states and counties lowercase.\n",
    "data1980.loc[:,\"State\"] = data1980.loc[:,\"State\"].str.lower()\n",
    "data1980.loc[:,\"County\"] = data1980.loc[:,\"County\"].str.lower()\n",
    "\n",
    "# Concatenate state name and county name as sta_con.\n",
    "data1980.loc[:,\"sta_con\"] = data1980.loc[:,\"State\"] + \"_\" + data1980.loc[:,\"County\"]\n",
    "data1 = pd.concat([inputdata,data1980])\n",
    "data1 = data1.drop(columns = ['County'])\n",
    "data1 = data1.merge(lon_lat,on=['sta_con'],how='inner',validate='many_to_one')\n",
    "# get the yield_N5_EXP2 column for 5 years平均产量\n",
    "data1 = average_yield_of_N_years(data1,5)\n",
    "# get the yield_N5_EXP3 column for 5 years线性产量（30年）\n",
    "data1 = linear_yield_of_N_years(data1,30)\n",
    "# get the yield_N5_EXP20 column for 30 years加上周围5个县的平均产量\n",
    "data1 = average_yield_of_N_years_nearest(data1,N_year = 5,N_nearest = 6)\n",
    "# get the yield_N5_EXP30 column for 30 years加上周围5个县的线性产量（30年）\n",
    "data1 = linear_yield_of_N_years_nearest(data1,N_year = 30,N_nearest = 6)\n",
    "# data1 = data1[data1['Year'].isin([i for i in range(2004,2022,1)])]\n",
    "data1.to_csv('D:/论文-产量趋势利用/数据/input_maize_exp.csv', index=False)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdea514-4195-4cff-9ead-33b3ea4e88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 玉米，测试平均年份\n",
    "# 添加经纬度列并计算试验列\n",
    "inputdata = read_data('D:/论文-产量趋势利用/数据/input_maize.csv')\n",
    "lon_lat = read_data('D:/论文-产量趋势利用/数据/区划数据/processed/lon_latitude.csv')\n",
    "# read yield data from 1980 to now\n",
    "data1980 = pd.read_csv(r'D:/论文-产量趋势利用/数据/产量数据/processed/maize/1980年以来的玉米单产.csv',engine = 'python')\n",
    "data1980 = data1980[data1980['Year'].isin([i for i in range(1980,2004,1)])]\n",
    "# Make the names of states and counties lowercase.\n",
    "data1980.loc[:,\"State\"] = data1980.loc[:,\"State\"].str.lower()\n",
    "data1980.loc[:,\"County\"] = data1980.loc[:,\"County\"].str.lower()\n",
    "\n",
    "# Concatenate state name and county name as sta_con.\n",
    "data1980.loc[:,\"sta_con\"] = data1980.loc[:,\"State\"] + \"_\" + data1980.loc[:,\"County\"]\n",
    "data1 = pd.concat([inputdata,data1980])\n",
    "data1 = data1.drop(columns = ['County'])\n",
    "data1 = data1.merge(lon_lat,on=['sta_con'],how='inner',validate='many_to_one')\n",
    "# get the yield_N5_EXP2 column for 5 years平均产量\n",
    "for i in range(2,35):\n",
    "    data1 = average_yield_of_N_years(data1,i)\n",
    "\n",
    "# data1 = data1[data1['Year'].isin([i for i in range(2004,2022,1)])]\n",
    "data1.to_csv('D:/论文-产量趋势利用/数据/input_maize_exp_ave.csv', index=False)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb1412-8b97-41d2-a5cd-264dbdb46c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 玉米，测试趋势年份\n",
    "# 添加经纬度列并计算试验列\n",
    "inputdata = read_data('D:/论文-产量趋势利用/数据/input_maize.csv')\n",
    "lon_lat = read_data('D:/论文-产量趋势利用/数据/区划数据/processed/lon_latitude.csv')\n",
    "# read yield data from 1980 to now\n",
    "data1980 = pd.read_csv(r'D:/论文-产量趋势利用/数据/产量数据/processed/maize/1980年以来的玉米单产.csv',engine = 'python')\n",
    "data1980 = data1980[data1980['Year'].isin([i for i in range(1980,2004,1)])]\n",
    "# Make the names of states and counties lowercase.\n",
    "data1980.loc[:,\"State\"] = data1980.loc[:,\"State\"].str.lower()\n",
    "data1980.loc[:,\"County\"] = data1980.loc[:,\"County\"].str.lower()\n",
    "\n",
    "# Concatenate state name and county name as sta_con.\n",
    "data1980.loc[:,\"sta_con\"] = data1980.loc[:,\"State\"] + \"_\" + data1980.loc[:,\"County\"]\n",
    "data1 = pd.concat([inputdata,data1980])\n",
    "data1 = data1.drop(columns = ['County'])\n",
    "data1 = data1.merge(lon_lat,on=['sta_con'],how='inner',validate='many_to_one')\n",
    "# get the yield_N5_EXP2 column for 5 years平均产量\n",
    "for i in range(2,35):\n",
    "    data1 = linear_yield_of_N_years(data1,i)\n",
    "\n",
    "# data1 = data1[data1['Year'].isin([i for i in range(2004,2022,1)])]\n",
    "data1.to_csv('D:/论文-产量趋势利用/数据/input_maize_exp_lin.csv', index=False)\n",
    "data1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
