{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import neighbors\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "import category_encoders as ce\n",
    "import math\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "seed_value= 0\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "seed(seed_value)\n",
    "np.random.seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    '''读取数据'''\n",
    "    try:\n",
    "        data = pd.read_csv(path, engine='python', encoding='utf8')\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print('File not found. Please check the path and filename.')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print('An error occurred:', e)\n",
    "        return None\n",
    "\n",
    "def assess_metrics(truth,pred):\n",
    "    '''MAE、MSE、RMSE、决定系数'''\n",
    "    print('----')\n",
    "    print('MAE: %.4f'%mean_absolute_error(truth,pred))\n",
    "    print('MSE: %.4f'%mean_squared_error(truth,pred))\n",
    "    print('RMSE: %.4f'%mean_squared_error(truth,pred,squared=False))\n",
    "    print('R2: %.4f'%r2_score(truth,pred))\n",
    "    return mean_squared_error(truth,pred,squared=False),r2_score(truth,pred)\n",
    "\n",
    "def model_result(est,X_train_val, X_test, y_train_val, y_test):\n",
    "    '''Train and test a model, and print the results'''\n",
    "    # training\n",
    "    y_train_pred = est.predict(X_train_val)\n",
    "    print('train metrics')\n",
    "    assess_metrics(y_train_val, y_train_pred)\n",
    "    # test\n",
    "    print('test metrics')\n",
    "    y_test_pred = est.predict(X_test)\n",
    "    RMSE, R2 = assess_metrics(y_test, y_test_pred)\n",
    "    return RMSE, R2\n",
    "def model_lineresult(est,model,year,X_train_val, X_test, y_train_val, y_test):\n",
    "    '''Train and test a linear regression model, and print the results in a line'''\n",
    "    date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    # train and test\n",
    "    y_train_pred = est.predict(X_train_val)\n",
    "    y_test_pred = est.predict(X_test)\n",
    "    print('{} {} {} 【Train_RMSE: {:.2f}, Train_R2: {:.2f}, Test_RMSE: {:.4f}, Test_R2: {:.2f}】'\n",
    "          .format(date,year,model,mean_squared_error(y_train_val, y_train_pred,squared=False),r2_score(y_train_val, y_train_pred),\n",
    "          mean_squared_error(y_test, y_test_pred,squared=False),r2_score(y_test, y_test_pred)))\n",
    "    return mean_squared_error(y_test, y_test_pred,squared=False),r2_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 大豆预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12119, 1240)\n",
      "['Year', 'State', 'Value', 'sta_con', 'yield(t/ha)', 'NDVI_Mid Mar.', 'EVI_Mid Mar.', 'LSWI_Mid Mar.', 'GCVI_Mid Mar.', 'RVI_Mid Mar.', 'SAVI_Mid Mar.', 'WDRVI_Mid Mar.', 'Fpar_Mid Mar.', 'LAI_Mid Mar.', 'ET_Mid Mar.', 'LE_Mid Mar.', 'LST_Day_Mid Mar.', 'LST_Night_Mid Mar.', 'SPI14d_Mid Mar.', 'SPI30d_Mid Mar.', 'SPI90d_Mid Mar.', 'EDDI14d_Mid Mar.', 'EDDI30d_Mid Mar.', 'EDDI90d_Mid Mar.', 'SPEI14d_Mid Mar.', 'SPEI30d_Mid Mar.', 'SPEI90d_Mid Mar.', 'PDSI_Mid Mar.', 'Palmer Z_Mid Mar.', 'Red_Mid Mar.', 'Nir_Mid Mar.', 'Blue_Mid Mar.', 'Green_Mid Mar.', 'Nir1_Mid Mar.', 'Swir1_Mid Mar.', 'Swir2_Mid Mar.', 'Precipitation_Mid Mar.', 'Temp_Mid Mar.', 'Humidity_Mid Mar.', 'Pressure_Mid Mar.', 'Shortwave_Mid Mar.', 'Longwave_Mid Mar.', 'NDVI_Late Mar.', 'EVI_Late Mar.', 'LSWI_Late Mar.', 'GCVI_Late Mar.', 'RVI_Late Mar.', 'SAVI_Late Mar.', 'WDRVI_Late Mar.', 'Fpar_Late Mar.', 'LAI_Late Mar.', 'ET_Late Mar.', 'LE_Late Mar.', 'LST_Day_Late Mar.', 'LST_Night_Late Mar.', 'SPI14d_Late Mar.', 'SPI30d_Late Mar.', 'SPI90d_Late Mar.', 'EDDI14d_Late Mar.', 'EDDI30d_Late Mar.', 'EDDI90d_Late Mar.', 'SPEI14d_Late Mar.', 'SPEI30d_Late Mar.', 'SPEI90d_Late Mar.', 'PDSI_Late Mar.', 'Palmer Z_Late Mar.', 'Red_Late Mar.', 'Nir_Late Mar.', 'Blue_Late Mar.', 'Green_Late Mar.', 'Nir1_Late Mar.', 'Swir1_Late Mar.', 'Swir2_Late Mar.', 'Precipitation_Late Mar.', 'Temp_Late Mar.', 'Humidity_Late Mar.', 'Pressure_Late Mar.', 'Shortwave_Late Mar.', 'Longwave_Late Mar.', 'NDVI_Early Apr.', 'EVI_Early Apr.', 'LSWI_Early Apr.', 'GCVI_Early Apr.', 'RVI_Early Apr.', 'SAVI_Early Apr.', 'WDRVI_Early Apr.', 'Fpar_Early Apr.', 'LAI_Early Apr.', 'ET_Early Apr.', 'LE_Early Apr.', 'LST_Day_Early Apr.', 'LST_Night_Early Apr.', 'SPI14d_Early Apr.', 'SPI30d_Early Apr.', 'SPI90d_Early Apr.', 'EDDI14d_Early Apr.', 'EDDI30d_Early Apr.', 'EDDI90d_Early Apr.', 'SPEI14d_Early Apr.', 'SPEI30d_Early Apr.', 'SPEI90d_Early Apr.', 'PDSI_Early Apr.', 'Palmer Z_Early Apr.', 'Red_Early Apr.', 'Nir_Early Apr.', 'Blue_Early Apr.', 'Green_Early Apr.', 'Nir1_Early Apr.', 'Swir1_Early Apr.', 'Swir2_Early Apr.', 'Precipitation_Early Apr.', 'Temp_Early Apr.', 'Humidity_Early Apr.', 'Pressure_Early Apr.', 'Shortwave_Early Apr.', 'Longwave_Early Apr.', 'NDVI_Mid Apr.', 'EVI_Mid Apr.', 'LSWI_Mid Apr.', 'GCVI_Mid Apr.', 'RVI_Mid Apr.', 'SAVI_Mid Apr.', 'WDRVI_Mid Apr.', 'Fpar_Mid Apr.', 'LAI_Mid Apr.', 'ET_Mid Apr.', 'LE_Mid Apr.', 'LST_Day_Mid Apr.', 'LST_Night_Mid Apr.', 'SPI14d_Mid Apr.', 'SPI30d_Mid Apr.', 'SPI90d_Mid Apr.', 'EDDI14d_Mid Apr.', 'EDDI30d_Mid Apr.', 'EDDI90d_Mid Apr.', 'SPEI14d_Mid Apr.', 'SPEI30d_Mid Apr.', 'SPEI90d_Mid Apr.', 'PDSI_Mid Apr.', 'Palmer Z_Mid Apr.', 'Red_Mid Apr.', 'Nir_Mid Apr.', 'Blue_Mid Apr.', 'Green_Mid Apr.', 'Nir1_Mid Apr.', 'Swir1_Mid Apr.', 'Swir2_Mid Apr.', 'Precipitation_Mid Apr.', 'Temp_Mid Apr.', 'Humidity_Mid Apr.', 'Pressure_Mid Apr.', 'Shortwave_Mid Apr.', 'Longwave_Mid Apr.', 'NDVI_Late Apr.', 'EVI_Late Apr.', 'LSWI_Late Apr.', 'GCVI_Late Apr.', 'RVI_Late Apr.', 'SAVI_Late Apr.', 'WDRVI_Late Apr.', 'Fpar_Late Apr.', 'LAI_Late Apr.', 'ET_Late Apr.', 'LE_Late Apr.', 'LST_Day_Late Apr.', 'LST_Night_Late Apr.', 'SPI14d_Late Apr.', 'SPI30d_Late Apr.', 'SPI90d_Late Apr.', 'EDDI14d_Late Apr.', 'EDDI30d_Late Apr.', 'EDDI90d_Late Apr.', 'SPEI14d_Late Apr.', 'SPEI30d_Late Apr.', 'SPEI90d_Late Apr.', 'PDSI_Late Apr.', 'Palmer Z_Late Apr.', 'Red_Late Apr.', 'Nir_Late Apr.', 'Blue_Late Apr.', 'Green_Late Apr.', 'Nir1_Late Apr.', 'Swir1_Late Apr.', 'Swir2_Late Apr.', 'Precipitation_Late Apr.', 'Temp_Late Apr.', 'Humidity_Late Apr.', 'Pressure_Late Apr.', 'Shortwave_Late Apr.', 'Longwave_Late Apr.', 'NDVI_Early May.', 'EVI_Early May.', 'LSWI_Early May.', 'GCVI_Early May.', 'RVI_Early May.', 'SAVI_Early May.', 'WDRVI_Early May.', 'Fpar_Early May.', 'LAI_Early May.', 'ET_Early May.', 'LE_Early May.', 'LST_Day_Early May.', 'LST_Night_Early May.', 'SPI14d_Early May.', 'SPI30d_Early May.', 'SPI90d_Early May.', 'EDDI14d_Early May.', 'EDDI30d_Early May.', 'EDDI90d_Early May.', 'SPEI14d_Early May.', 'SPEI30d_Early May.', 'SPEI90d_Early May.', 'PDSI_Early May.', 'Palmer Z_Early May.', 'Red_Early May.', 'Nir_Early May.', 'Blue_Early May.', 'Green_Early May.', 'Nir1_Early May.', 'Swir1_Early May.', 'Swir2_Early May.', 'Precipitation_Early May.', 'Temp_Early May.', 'Humidity_Early May.', 'Pressure_Early May.', 'Shortwave_Early May.', 'Longwave_Early May.', 'NDVI_Mid May.', 'EVI_Mid May.', 'LSWI_Mid May.', 'GCVI_Mid May.', 'RVI_Mid May.', 'SAVI_Mid May.', 'WDRVI_Mid May.', 'Fpar_Mid May.', 'LAI_Mid May.', 'ET_Mid May.', 'LE_Mid May.', 'LST_Day_Mid May.', 'LST_Night_Mid May.', 'SPI14d_Mid May.', 'SPI30d_Mid May.', 'SPI90d_Mid May.', 'EDDI14d_Mid May.', 'EDDI30d_Mid May.', 'EDDI90d_Mid May.', 'SPEI14d_Mid May.', 'SPEI30d_Mid May.', 'SPEI90d_Mid May.', 'PDSI_Mid May.', 'Palmer Z_Mid May.', 'Red_Mid May.', 'Nir_Mid May.', 'Blue_Mid May.', 'Green_Mid May.', 'Nir1_Mid May.', 'Swir1_Mid May.', 'Swir2_Mid May.', 'Precipitation_Mid May.', 'Temp_Mid May.', 'Humidity_Mid May.', 'Pressure_Mid May.', 'Shortwave_Mid May.', 'Longwave_Mid May.', 'NDVI_Late May.', 'EVI_Late May.', 'LSWI_Late May.', 'GCVI_Late May.', 'RVI_Late May.', 'SAVI_Late May.', 'WDRVI_Late May.', 'Fpar_Late May.', 'LAI_Late May.', 'ET_Late May.', 'LE_Late May.', 'LST_Day_Late May.', 'LST_Night_Late May.', 'SPI14d_Late May.', 'SPI30d_Late May.', 'SPI90d_Late May.', 'EDDI14d_Late May.', 'EDDI30d_Late May.', 'EDDI90d_Late May.', 'SPEI14d_Late May.', 'SPEI30d_Late May.', 'SPEI90d_Late May.', 'PDSI_Late May.', 'Palmer Z_Late May.', 'Red_Late May.', 'Nir_Late May.', 'Blue_Late May.', 'Green_Late May.', 'Nir1_Late May.', 'Swir1_Late May.', 'Swir2_Late May.', 'Precipitation_Late May.', 'Temp_Late May.', 'Humidity_Late May.', 'Pressure_Late May.', 'Shortwave_Late May.', 'Longwave_Late May.', 'NDVI_Early Jun.', 'EVI_Early Jun.', 'LSWI_Early Jun.', 'GCVI_Early Jun.', 'RVI_Early Jun.', 'SAVI_Early Jun.', 'WDRVI_Early Jun.', 'Fpar_Early Jun.', 'LAI_Early Jun.', 'ET_Early Jun.', 'LE_Early Jun.', 'LST_Day_Early Jun.', 'LST_Night_Early Jun.', 'SPI14d_Early Jun.', 'SPI30d_Early Jun.', 'SPI90d_Early Jun.', 'EDDI14d_Early Jun.', 'EDDI30d_Early Jun.', 'EDDI90d_Early Jun.', 'SPEI14d_Early Jun.', 'SPEI30d_Early Jun.', 'SPEI90d_Early Jun.', 'PDSI_Early Jun.', 'Palmer Z_Early Jun.', 'Red_Early Jun.', 'Nir_Early Jun.', 'Blue_Early Jun.', 'Green_Early Jun.', 'Nir1_Early Jun.', 'Swir1_Early Jun.', 'Swir2_Early Jun.', 'Precipitation_Early Jun.', 'Temp_Early Jun.', 'Humidity_Early Jun.', 'Pressure_Early Jun.', 'Shortwave_Early Jun.', 'Longwave_Early Jun.', 'NDVI_Mid Jun.', 'EVI_Mid Jun.', 'LSWI_Mid Jun.', 'GCVI_Mid Jun.', 'RVI_Mid Jun.', 'SAVI_Mid Jun.', 'WDRVI_Mid Jun.', 'Fpar_Mid Jun.', 'LAI_Mid Jun.', 'ET_Mid Jun.', 'LE_Mid Jun.', 'LST_Day_Mid Jun.', 'LST_Night_Mid Jun.', 'SPI14d_Mid Jun.', 'SPI30d_Mid Jun.', 'SPI90d_Mid Jun.', 'EDDI14d_Mid Jun.', 'EDDI30d_Mid Jun.', 'EDDI90d_Mid Jun.', 'SPEI14d_Mid Jun.', 'SPEI30d_Mid Jun.', 'SPEI90d_Mid Jun.', 'PDSI_Mid Jun.', 'Palmer Z_Mid Jun.', 'Red_Mid Jun.', 'Nir_Mid Jun.', 'Blue_Mid Jun.', 'Green_Mid Jun.', 'Nir1_Mid Jun.', 'Swir1_Mid Jun.', 'Swir2_Mid Jun.', 'Precipitation_Mid Jun.', 'Temp_Mid Jun.', 'Humidity_Mid Jun.', 'Pressure_Mid Jun.', 'Shortwave_Mid Jun.', 'Longwave_Mid Jun.', 'NDVI_Late Jun.', 'EVI_Late Jun.', 'LSWI_Late Jun.', 'GCVI_Late Jun.', 'RVI_Late Jun.', 'SAVI_Late Jun.', 'WDRVI_Late Jun.', 'Fpar_Late Jun.', 'LAI_Late Jun.', 'ET_Late Jun.', 'LE_Late Jun.', 'LST_Day_Late Jun.', 'LST_Night_Late Jun.', 'SPI14d_Late Jun.', 'SPI30d_Late Jun.', 'SPI90d_Late Jun.', 'EDDI14d_Late Jun.', 'EDDI30d_Late Jun.', 'EDDI90d_Late Jun.', 'SPEI14d_Late Jun.', 'SPEI30d_Late Jun.', 'SPEI90d_Late Jun.', 'PDSI_Late Jun.', 'Palmer Z_Late Jun.', 'Red_Late Jun.', 'Nir_Late Jun.', 'Blue_Late Jun.', 'Green_Late Jun.', 'Nir1_Late Jun.', 'Swir1_Late Jun.', 'Swir2_Late Jun.', 'Precipitation_Late Jun.', 'Temp_Late Jun.', 'Humidity_Late Jun.', 'Pressure_Late Jun.', 'Shortwave_Late Jun.', 'Longwave_Late Jun.', 'NDVI_Early Jul.', 'EVI_Early Jul.', 'LSWI_Early Jul.', 'GCVI_Early Jul.', 'RVI_Early Jul.', 'SAVI_Early Jul.', 'WDRVI_Early Jul.', 'Fpar_Early Jul.', 'LAI_Early Jul.', 'ET_Early Jul.', 'LE_Early Jul.', 'LST_Day_Early Jul.', 'LST_Night_Early Jul.', 'SPI14d_Early Jul.', 'SPI30d_Early Jul.', 'SPI90d_Early Jul.', 'EDDI14d_Early Jul.', 'EDDI30d_Early Jul.', 'EDDI90d_Early Jul.', 'SPEI14d_Early Jul.', 'SPEI30d_Early Jul.', 'SPEI90d_Early Jul.', 'PDSI_Early Jul.', 'Palmer Z_Early Jul.', 'Red_Early Jul.', 'Nir_Early Jul.', 'Blue_Early Jul.', 'Green_Early Jul.', 'Nir1_Early Jul.', 'Swir1_Early Jul.', 'Swir2_Early Jul.', 'Precipitation_Early Jul.', 'Temp_Early Jul.', 'Humidity_Early Jul.', 'Pressure_Early Jul.', 'Shortwave_Early Jul.', 'Longwave_Early Jul.', 'NDVI_Mid Jul.', 'EVI_Mid Jul.', 'LSWI_Mid Jul.', 'GCVI_Mid Jul.', 'RVI_Mid Jul.', 'SAVI_Mid Jul.', 'WDRVI_Mid Jul.', 'Fpar_Mid Jul.', 'LAI_Mid Jul.', 'ET_Mid Jul.', 'LE_Mid Jul.', 'LST_Day_Mid Jul.', 'LST_Night_Mid Jul.', 'SPI14d_Mid Jul.', 'SPI30d_Mid Jul.', 'SPI90d_Mid Jul.', 'EDDI14d_Mid Jul.', 'EDDI30d_Mid Jul.', 'EDDI90d_Mid Jul.', 'SPEI14d_Mid Jul.', 'SPEI30d_Mid Jul.', 'SPEI90d_Mid Jul.', 'PDSI_Mid Jul.', 'Palmer Z_Mid Jul.', 'Red_Mid Jul.', 'Nir_Mid Jul.', 'Blue_Mid Jul.', 'Green_Mid Jul.', 'Nir1_Mid Jul.', 'Swir1_Mid Jul.', 'Swir2_Mid Jul.', 'Precipitation_Mid Jul.', 'Temp_Mid Jul.', 'Humidity_Mid Jul.', 'Pressure_Mid Jul.', 'Shortwave_Mid Jul.', 'Longwave_Mid Jul.', 'NDVI_Late Jul.', 'EVI_Late Jul.', 'LSWI_Late Jul.', 'GCVI_Late Jul.', 'RVI_Late Jul.', 'SAVI_Late Jul.', 'WDRVI_Late Jul.', 'Fpar_Late Jul.', 'LAI_Late Jul.', 'ET_Late Jul.', 'LE_Late Jul.', 'LST_Day_Late Jul.', 'LST_Night_Late Jul.', 'SPI14d_Late Jul.', 'SPI30d_Late Jul.', 'SPI90d_Late Jul.', 'EDDI14d_Late Jul.', 'EDDI30d_Late Jul.', 'EDDI90d_Late Jul.', 'SPEI14d_Late Jul.', 'SPEI30d_Late Jul.', 'SPEI90d_Late Jul.', 'PDSI_Late Jul.', 'Palmer Z_Late Jul.', 'Red_Late Jul.', 'Nir_Late Jul.', 'Blue_Late Jul.', 'Green_Late Jul.', 'Nir1_Late Jul.', 'Swir1_Late Jul.', 'Swir2_Late Jul.', 'Precipitation_Late Jul.', 'Temp_Late Jul.', 'Humidity_Late Jul.', 'Pressure_Late Jul.', 'Shortwave_Late Jul.', 'Longwave_Late Jul.', 'NDVI_Early Aug.', 'EVI_Early Aug.', 'LSWI_Early Aug.', 'GCVI_Early Aug.', 'RVI_Early Aug.', 'SAVI_Early Aug.', 'WDRVI_Early Aug.', 'Fpar_Early Aug.', 'LAI_Early Aug.', 'ET_Early Aug.', 'LE_Early Aug.', 'LST_Day_Early Aug.', 'LST_Night_Early Aug.', 'SPI14d_Early Aug.', 'SPI30d_Early Aug.', 'SPI90d_Early Aug.', 'EDDI14d_Early Aug.', 'EDDI30d_Early Aug.', 'EDDI90d_Early Aug.', 'SPEI14d_Early Aug.', 'SPEI30d_Early Aug.', 'SPEI90d_Early Aug.', 'PDSI_Early Aug.', 'Palmer Z_Early Aug.', 'Red_Early Aug.', 'Nir_Early Aug.', 'Blue_Early Aug.', 'Green_Early Aug.', 'Nir1_Early Aug.', 'Swir1_Early Aug.', 'Swir2_Early Aug.', 'Precipitation_Early Aug.', 'Temp_Early Aug.', 'Humidity_Early Aug.', 'Pressure_Early Aug.', 'Shortwave_Early Aug.', 'Longwave_Early Aug.', 'NDVI_Mid Aug.', 'EVI_Mid Aug.', 'LSWI_Mid Aug.', 'GCVI_Mid Aug.', 'RVI_Mid Aug.', 'SAVI_Mid Aug.', 'WDRVI_Mid Aug.', 'Fpar_Mid Aug.', 'LAI_Mid Aug.', 'ET_Mid Aug.', 'LE_Mid Aug.', 'LST_Day_Mid Aug.', 'LST_Night_Mid Aug.', 'SPI14d_Mid Aug.', 'SPI30d_Mid Aug.', 'SPI90d_Mid Aug.', 'EDDI14d_Mid Aug.', 'EDDI30d_Mid Aug.', 'EDDI90d_Mid Aug.', 'SPEI14d_Mid Aug.', 'SPEI30d_Mid Aug.', 'SPEI90d_Mid Aug.', 'PDSI_Mid Aug.', 'Palmer Z_Mid Aug.', 'Red_Mid Aug.', 'Nir_Mid Aug.', 'Blue_Mid Aug.', 'Green_Mid Aug.', 'Nir1_Mid Aug.', 'Swir1_Mid Aug.', 'Swir2_Mid Aug.', 'Precipitation_Mid Aug.', 'Temp_Mid Aug.', 'Humidity_Mid Aug.', 'Pressure_Mid Aug.', 'Shortwave_Mid Aug.', 'Longwave_Mid Aug.', 'NDVI_Late Aug.', 'EVI_Late Aug.', 'LSWI_Late Aug.', 'GCVI_Late Aug.', 'RVI_Late Aug.', 'SAVI_Late Aug.', 'WDRVI_Late Aug.', 'Fpar_Late Aug.', 'LAI_Late Aug.', 'ET_Late Aug.', 'LE_Late Aug.', 'LST_Day_Late Aug.', 'LST_Night_Late Aug.', 'SPI14d_Late Aug.', 'SPI30d_Late Aug.', 'SPI90d_Late Aug.', 'EDDI14d_Late Aug.', 'EDDI30d_Late Aug.', 'EDDI90d_Late Aug.', 'SPEI14d_Late Aug.', 'SPEI30d_Late Aug.', 'SPEI90d_Late Aug.', 'PDSI_Late Aug.', 'Palmer Z_Late Aug.', 'Red_Late Aug.', 'Nir_Late Aug.', 'Blue_Late Aug.', 'Green_Late Aug.', 'Nir1_Late Aug.', 'Swir1_Late Aug.', 'Swir2_Late Aug.', 'Precipitation_Late Aug.', 'Temp_Late Aug.', 'Humidity_Late Aug.', 'Pressure_Late Aug.', 'Shortwave_Late Aug.', 'Longwave_Late Aug.', 'NDVI_Early Sep.', 'EVI_Early Sep.', 'LSWI_Early Sep.', 'GCVI_Early Sep.', 'RVI_Early Sep.', 'SAVI_Early Sep.', 'WDRVI_Early Sep.', 'Fpar_Early Sep.', 'LAI_Early Sep.', 'ET_Early Sep.', 'LE_Early Sep.', 'LST_Day_Early Sep.', 'LST_Night_Early Sep.', 'SPI14d_Early Sep.', 'SPI30d_Early Sep.', 'SPI90d_Early Sep.', 'EDDI14d_Early Sep.', 'EDDI30d_Early Sep.', 'EDDI90d_Early Sep.', 'SPEI14d_Early Sep.', 'SPEI30d_Early Sep.', 'SPEI90d_Early Sep.', 'PDSI_Early Sep.', 'Palmer Z_Early Sep.', 'Red_Early Sep.', 'Nir_Early Sep.', 'Blue_Early Sep.', 'Green_Early Sep.', 'Nir1_Early Sep.', 'Swir1_Early Sep.', 'Swir2_Early Sep.', 'Precipitation_Early Sep.', 'Temp_Early Sep.', 'Humidity_Early Sep.', 'Pressure_Early Sep.', 'Shortwave_Early Sep.', 'Longwave_Early Sep.', 'NDVI_Mid Sep.', 'EVI_Mid Sep.', 'LSWI_Mid Sep.', 'GCVI_Mid Sep.', 'RVI_Mid Sep.', 'SAVI_Mid Sep.', 'WDRVI_Mid Sep.', 'Fpar_Mid Sep.', 'LAI_Mid Sep.', 'ET_Mid Sep.', 'LE_Mid Sep.', 'LST_Day_Mid Sep.', 'LST_Night_Mid Sep.', 'SPI14d_Mid Sep.', 'SPI30d_Mid Sep.', 'SPI90d_Mid Sep.', 'EDDI14d_Mid Sep.', 'EDDI30d_Mid Sep.', 'EDDI90d_Mid Sep.', 'SPEI14d_Mid Sep.', 'SPEI30d_Mid Sep.', 'SPEI90d_Mid Sep.', 'PDSI_Mid Sep.', 'Palmer Z_Mid Sep.', 'Red_Mid Sep.', 'Nir_Mid Sep.', 'Blue_Mid Sep.', 'Green_Mid Sep.', 'Nir1_Mid Sep.', 'Swir1_Mid Sep.', 'Swir2_Mid Sep.', 'Precipitation_Mid Sep.', 'Temp_Mid Sep.', 'Humidity_Mid Sep.', 'Pressure_Mid Sep.', 'Shortwave_Mid Sep.', 'Longwave_Mid Sep.', 'NDVI_Late Sep.', 'EVI_Late Sep.', 'LSWI_Late Sep.', 'GCVI_Late Sep.', 'RVI_Late Sep.', 'SAVI_Late Sep.', 'WDRVI_Late Sep.', 'Fpar_Late Sep.', 'LAI_Late Sep.', 'ET_Late Sep.', 'LE_Late Sep.', 'LST_Day_Late Sep.', 'LST_Night_Late Sep.', 'SPI14d_Late Sep.', 'SPI30d_Late Sep.', 'SPI90d_Late Sep.', 'EDDI14d_Late Sep.', 'EDDI30d_Late Sep.', 'EDDI90d_Late Sep.', 'SPEI14d_Late Sep.', 'SPEI30d_Late Sep.', 'SPEI90d_Late Sep.', 'PDSI_Late Sep.', 'Palmer Z_Late Sep.', 'Red_Late Sep.', 'Nir_Late Sep.', 'Blue_Late Sep.', 'Green_Late Sep.', 'Nir1_Late Sep.', 'Swir1_Late Sep.', 'Swir2_Late Sep.', 'Precipitation_Late Sep.', 'Temp_Late Sep.', 'Humidity_Late Sep.', 'Pressure_Late Sep.', 'Shortwave_Late Sep.', 'Longwave_Late Sep.', 'NDVI_Early Oct.', 'EVI_Early Oct.', 'LSWI_Early Oct.', 'GCVI_Early Oct.', 'RVI_Early Oct.', 'SAVI_Early Oct.', 'WDRVI_Early Oct.', 'Fpar_Early Oct.', 'LAI_Early Oct.', 'ET_Early Oct.', 'LE_Early Oct.', 'LST_Day_Early Oct.', 'LST_Night_Early Oct.', 'SPI14d_Early Oct.', 'SPI30d_Early Oct.', 'SPI90d_Early Oct.', 'EDDI14d_Early Oct.', 'EDDI30d_Early Oct.', 'EDDI90d_Early Oct.', 'SPEI14d_Early Oct.', 'SPEI30d_Early Oct.', 'SPEI90d_Early Oct.', 'PDSI_Early Oct.', 'Palmer Z_Early Oct.', 'Red_Early Oct.', 'Nir_Early Oct.', 'Blue_Early Oct.', 'Green_Early Oct.', 'Nir1_Early Oct.', 'Swir1_Early Oct.', 'Swir2_Early Oct.', 'Precipitation_Early Oct.', 'Temp_Early Oct.', 'Humidity_Early Oct.', 'Pressure_Early Oct.', 'Shortwave_Early Oct.', 'Longwave_Early Oct.', 'NDVI_Mid Oct.', 'EVI_Mid Oct.', 'LSWI_Mid Oct.', 'GCVI_Mid Oct.', 'RVI_Mid Oct.', 'SAVI_Mid Oct.', 'WDRVI_Mid Oct.', 'Fpar_Mid Oct.', 'LAI_Mid Oct.', 'ET_Mid Oct.', 'LE_Mid Oct.', 'LST_Day_Mid Oct.', 'LST_Night_Mid Oct.', 'SPI14d_Mid Oct.', 'SPI30d_Mid Oct.', 'SPI90d_Mid Oct.', 'EDDI14d_Mid Oct.', 'EDDI30d_Mid Oct.', 'EDDI90d_Mid Oct.', 'SPEI14d_Mid Oct.', 'SPEI30d_Mid Oct.', 'SPEI90d_Mid Oct.', 'PDSI_Mid Oct.', 'Palmer Z_Mid Oct.', 'Red_Mid Oct.', 'Nir_Mid Oct.', 'Blue_Mid Oct.', 'Green_Mid Oct.', 'Nir1_Mid Oct.', 'Swir1_Mid Oct.', 'Swir2_Mid Oct.', 'Precipitation_Mid Oct.', 'Temp_Mid Oct.', 'Humidity_Mid Oct.', 'Pressure_Mid Oct.', 'Shortwave_Mid Oct.', 'Longwave_Mid Oct.', 'NDVI_Late Oct.', 'EVI_Late Oct.', 'LSWI_Late Oct.', 'GCVI_Late Oct.', 'RVI_Late Oct.', 'SAVI_Late Oct.', 'WDRVI_Late Oct.', 'Fpar_Late Oct.', 'LAI_Late Oct.', 'ET_Late Oct.', 'LE_Late Oct.', 'LST_Day_Late Oct.', 'LST_Night_Late Oct.', 'SPI14d_Late Oct.', 'SPI30d_Late Oct.', 'SPI90d_Late Oct.', 'EDDI14d_Late Oct.', 'EDDI30d_Late Oct.', 'EDDI90d_Late Oct.', 'SPEI14d_Late Oct.', 'SPEI30d_Late Oct.', 'SPEI90d_Late Oct.', 'PDSI_Late Oct.', 'Palmer Z_Late Oct.', 'Red_Late Oct.', 'Nir_Late Oct.', 'Blue_Late Oct.', 'Green_Late Oct.', 'Nir1_Late Oct.', 'Swir1_Late Oct.', 'Swir2_Late Oct.', 'Precipitation_Late Oct.', 'Temp_Late Oct.', 'Humidity_Late Oct.', 'Pressure_Late Oct.', 'Shortwave_Late Oct.', 'Longwave_Late Oct.', 'NDVI_Early Nov.', 'EVI_Early Nov.', 'LSWI_Early Nov.', 'GCVI_Early Nov.', 'RVI_Early Nov.', 'SAVI_Early Nov.', 'WDRVI_Early Nov.', 'Fpar_Early Nov.', 'LAI_Early Nov.', 'ET_Early Nov.', 'LE_Early Nov.', 'LST_Day_Early Nov.', 'LST_Night_Early Nov.', 'SPI14d_Early Nov.', 'SPI30d_Early Nov.', 'SPI90d_Early Nov.', 'EDDI14d_Early Nov.', 'EDDI30d_Early Nov.', 'EDDI90d_Early Nov.', 'SPEI14d_Early Nov.', 'SPEI30d_Early Nov.', 'SPEI90d_Early Nov.', 'PDSI_Early Nov.', 'Palmer Z_Early Nov.', 'Red_Early Nov.', 'Nir_Early Nov.', 'Blue_Early Nov.', 'Green_Early Nov.', 'Nir1_Early Nov.', 'Swir1_Early Nov.', 'Swir2_Early Nov.', 'Precipitation_Early Nov.', 'Temp_Early Nov.', 'Humidity_Early Nov.', 'Pressure_Early Nov.', 'Shortwave_Early Nov.', 'Longwave_Early Nov.', 'NDVI_Mid Nov.', 'EVI_Mid Nov.', 'LSWI_Mid Nov.', 'GCVI_Mid Nov.', 'RVI_Mid Nov.', 'SAVI_Mid Nov.', 'WDRVI_Mid Nov.', 'Fpar_Mid Nov.', 'LAI_Mid Nov.', 'ET_Mid Nov.', 'LE_Mid Nov.', 'LST_Day_Mid Nov.', 'LST_Night_Mid Nov.', 'SPI14d_Mid Nov.', 'SPI30d_Mid Nov.', 'SPI90d_Mid Nov.', 'EDDI14d_Mid Nov.', 'EDDI30d_Mid Nov.', 'EDDI90d_Mid Nov.', 'SPEI14d_Mid Nov.', 'SPEI30d_Mid Nov.', 'SPEI90d_Mid Nov.', 'PDSI_Mid Nov.', 'Palmer Z_Mid Nov.', 'Red_Mid Nov.', 'Nir_Mid Nov.', 'Blue_Mid Nov.', 'Green_Mid Nov.', 'Nir1_Mid Nov.', 'Swir1_Mid Nov.', 'Swir2_Mid Nov.', 'Precipitation_Mid Nov.', 'Temp_Mid Nov.', 'Humidity_Mid Nov.', 'Pressure_Mid Nov.', 'Shortwave_Mid Nov.', 'Longwave_Mid Nov.', 'NDVI_Late Nov.', 'EVI_Late Nov.', 'LSWI_Late Nov.', 'GCVI_Late Nov.', 'RVI_Late Nov.', 'SAVI_Late Nov.', 'WDRVI_Late Nov.', 'Fpar_Late Nov.', 'LAI_Late Nov.', 'ET_Late Nov.', 'LE_Late Nov.', 'LST_Day_Late Nov.', 'LST_Night_Late Nov.', 'SPI14d_Late Nov.', 'SPI30d_Late Nov.', 'SPI90d_Late Nov.', 'EDDI14d_Late Nov.', 'EDDI30d_Late Nov.', 'EDDI90d_Late Nov.', 'SPEI14d_Late Nov.', 'SPEI30d_Late Nov.', 'SPEI90d_Late Nov.', 'PDSI_Late Nov.', 'Palmer Z_Late Nov.', 'Red_Late Nov.', 'Nir_Late Nov.', 'Blue_Late Nov.', 'Green_Late Nov.', 'Nir1_Late Nov.', 'Swir1_Late Nov.', 'Swir2_Late Nov.', 'Precipitation_Late Nov.', 'Temp_Late Nov.', 'Humidity_Late Nov.', 'Pressure_Late Nov.', 'Shortwave_Late Nov.', 'Longwave_Late Nov.', 'NDVI_Early Dec.', 'EVI_Early Dec.', 'LSWI_Early Dec.', 'GCVI_Early Dec.', 'RVI_Early Dec.', 'SAVI_Early Dec.', 'WDRVI_Early Dec.', 'Fpar_Early Dec.', 'LAI_Early Dec.', 'ET_Early Dec.', 'LE_Early Dec.', 'LST_Day_Early Dec.', 'LST_Night_Early Dec.', 'SPI14d_Early Dec.', 'SPI30d_Early Dec.', 'SPI90d_Early Dec.', 'EDDI14d_Early Dec.', 'EDDI30d_Early Dec.', 'EDDI90d_Early Dec.', 'SPEI14d_Early Dec.', 'SPEI30d_Early Dec.', 'SPEI90d_Early Dec.', 'PDSI_Early Dec.', 'Palmer Z_Early Dec.', 'Red_Early Dec.', 'Nir_Early Dec.', 'Blue_Early Dec.', 'Green_Early Dec.', 'Nir1_Early Dec.', 'Swir1_Early Dec.', 'Swir2_Early Dec.', 'Precipitation_Early Dec.', 'Temp_Early Dec.', 'Humidity_Early Dec.', 'Pressure_Early Dec.', 'Shortwave_Early Dec.', 'Longwave_Early Dec.', 'NDVI_Planting', 'EVI_Planting', 'LSWI_Planting', 'GCVI_Planting', 'RVI_Planting', 'SAVI_Planting', 'WDRVI_Planting', 'Fpar_Planting', 'LAI_Planting', 'LE_Planting', 'LST_Day_Planting', 'LST_Night_Planting', 'SPI14d_Planting', 'SPI30d_Planting', 'SPI90d_Planting', 'EDDI14d_Planting', 'EDDI30d_Planting', 'EDDI90d_Planting', 'SPEI14d_Planting', 'SPEI30d_Planting', 'SPEI90d_Planting', 'PDSI_Planting', 'Palmer Z_Planting', 'Red_Planting', 'Nir_Planting', 'Blue_Planting', 'Green_Planting', 'Nir1_Planting', 'Swir1_Planting', 'Swir2_Planting', 'NDVI_Emerging', 'EVI_Emerging', 'LSWI_Emerging', 'GCVI_Emerging', 'RVI_Emerging', 'SAVI_Emerging', 'WDRVI_Emerging', 'Fpar_Emerging', 'LAI_Emerging', 'LE_Emerging', 'LST_Day_Emerging', 'LST_Night_Emerging', 'SPI14d_Emerging', 'SPI30d_Emerging', 'SPI90d_Emerging', 'EDDI14d_Emerging', 'EDDI30d_Emerging', 'EDDI90d_Emerging', 'SPEI14d_Emerging', 'SPEI30d_Emerging', 'SPEI90d_Emerging', 'PDSI_Emerging', 'Palmer Z_Emerging', 'Red_Emerging', 'Nir_Emerging', 'Blue_Emerging', 'Green_Emerging', 'Nir1_Emerging', 'Swir1_Emerging', 'Swir2_Emerging', 'NDVI_Blooming', 'EVI_Blooming', 'LSWI_Blooming', 'GCVI_Blooming', 'RVI_Blooming', 'SAVI_Blooming', 'WDRVI_Blooming', 'Fpar_Blooming', 'LAI_Blooming', 'LE_Blooming', 'LST_Day_Blooming', 'LST_Night_Blooming', 'SPI14d_Blooming', 'SPI30d_Blooming', 'SPI90d_Blooming', 'EDDI14d_Blooming', 'EDDI30d_Blooming', 'EDDI90d_Blooming', 'SPEI14d_Blooming', 'SPEI30d_Blooming', 'SPEI90d_Blooming', 'PDSI_Blooming', 'Palmer Z_Blooming', 'Red_Blooming', 'Nir_Blooming', 'Blue_Blooming', 'Green_Blooming', 'Nir1_Blooming', 'Swir1_Blooming', 'Swir2_Blooming', 'NDVI_Podding', 'EVI_Podding', 'LSWI_Podding', 'GCVI_Podding', 'RVI_Podding', 'SAVI_Podding', 'WDRVI_Podding', 'Fpar_Podding', 'LAI_Podding', 'LE_Podding', 'LST_Day_Podding', 'LST_Night_Podding', 'SPI14d_Podding', 'SPI30d_Podding', 'SPI90d_Podding', 'EDDI14d_Podding', 'EDDI30d_Podding', 'EDDI90d_Podding', 'SPEI14d_Podding', 'SPEI30d_Podding', 'SPEI90d_Podding', 'PDSI_Podding', 'Palmer Z_Podding', 'Red_Podding', 'Nir_Podding', 'Blue_Podding', 'Green_Podding', 'Nir1_Podding', 'Swir1_Podding', 'Swir2_Podding', 'NDVI_Dropping leaves', 'EVI_Dropping leaves', 'LSWI_Dropping leaves', 'GCVI_Dropping leaves', 'RVI_Dropping leaves', 'SAVI_Dropping leaves', 'WDRVI_Dropping leaves', 'Fpar_Dropping leaves', 'LAI_Dropping leaves', 'LE_Dropping leaves', 'LST_Day_Dropping leaves', 'LST_Night_Dropping leaves', 'SPI14d_Dropping leaves', 'SPI30d_Dropping leaves', 'SPI90d_Dropping leaves', 'EDDI14d_Dropping leaves', 'EDDI30d_Dropping leaves', 'EDDI90d_Dropping leaves', 'SPEI14d_Dropping leaves', 'SPEI30d_Dropping leaves', 'SPEI90d_Dropping leaves', 'PDSI_Dropping leaves', 'Palmer Z_Dropping leaves', 'Red_Dropping leaves', 'Nir_Dropping leaves', 'Blue_Dropping leaves', 'Green_Dropping leaves', 'Nir1_Dropping leaves', 'Swir1_Dropping leaves', 'Swir2_Dropping leaves', 'NDVI_Harvest', 'EVI_Harvest', 'LSWI_Harvest', 'GCVI_Harvest', 'RVI_Harvest', 'SAVI_Harvest', 'WDRVI_Harvest', 'Fpar_Harvest', 'LAI_Harvest', 'LE_Harvest', 'LST_Day_Harvest', 'LST_Night_Harvest', 'SPI14d_Harvest', 'SPI30d_Harvest', 'SPI90d_Harvest', 'EDDI14d_Harvest', 'EDDI30d_Harvest', 'EDDI90d_Harvest', 'SPEI14d_Harvest', 'SPEI30d_Harvest', 'SPEI90d_Harvest', 'PDSI_Harvest', 'Palmer Z_Harvest', 'Red_Harvest', 'Nir_Harvest', 'Blue_Harvest', 'Green_Harvest', 'Nir1_Harvest', 'Swir1_Harvest', 'Swir2_Harvest', 'Temp_Planting', 'Shortwave_Planting', 'Longwave_Planting', 'ET_Planting', 'Temp_Emerging', 'Shortwave_Emerging', 'Longwave_Emerging', 'ET_Emerging', 'Temp_Blooming', 'Shortwave_Blooming', 'Longwave_Blooming', 'ET_Blooming', 'Temp_Podding', 'Shortwave_Podding', 'Longwave_Podding', 'ET_Podding', 'Temp_Dropping leaves', 'Shortwave_Dropping leaves', 'Longwave_Dropping leaves', 'ET_Dropping leaves', 'Temp_Harvest', 'Shortwave_Harvest', 'Longwave_Harvest', 'ET_Harvest', 'AREA_Irrigated', 'AREA', 'PIC', 'CaCO3', 'CEC', 'Drainage', 'EC', 'I_class', 'N_class', 'Max_OM', 'PAWS', 'pH', 'SAR', 'Texture', 'Sand', 'Silt', 'Clay', 'Longitude', 'Latitude', 'GDT_trend_yield', 'IL', 'IN', 'IA', 'KS', 'MI', 'MN', 'MO', 'NE', 'ND', 'OH', 'SD', 'WI']\n"
     ]
    }
   ],
   "source": [
    "# 大豆预测\n",
    "# read data\n",
    "# 2014-2021，每10年预测一年\n",
    "data = read_data('D:/毕业大论文/数据/input_soybean_rename.csv')\n",
    "\n",
    "# 删掉空缺值\n",
    "data = data.dropna()\n",
    "\n",
    "# 打印数据维度\n",
    "print(data.shape)\n",
    "\n",
    "# 打印特征列表区分时间特征、物候特征、静态特征\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('时间特征维度：',)\n",
    "print('物候特征维度：')\n",
    "print('静态特征维度：',len(['AREA_Irrigated', 'AREA', 'PIC', \n",
    "                     'CaCO3', 'CEC', 'Drainage', 'EC', 'I_class', 'N_class', 'Max_OM', 'PAWS', 'pH', 'SAR', 'Texture', 'Sand', 'Silt', 'Clay', \n",
    "                     'Longitude', 'Latitude', \n",
    "                     'IL', 'IN', 'IA', 'KS', 'MI', 'MN', 'MO', 'NE', 'ND', 'OH', 'SD', 'WI']))\n",
    "# 多余的应该删去的 'sta_con','State','yield(t/ha)','Year','Value','GDT_trend_yield'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始DataFrame：\n",
      "   A  B      C\n",
      "0  1  a   True\n",
      "1  2  b  False\n",
      "2  3  c   True\n",
      "变换后的DataFrame：\n",
      "       C  A  B\n",
      "0   True  1  a\n",
      "1  False  2  b\n",
      "2   True  3  c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 构造一个示例DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': ['a', 'b', 'c'],\n",
    "    'C': [True, False, True],\n",
    "})\n",
    "\n",
    "# 打印原始DataFrame\n",
    "print('原始DataFrame：')\n",
    "print(df)\n",
    "\n",
    "# 定义新的列顺序\n",
    "new_columns = ['C', 'A', 'B']\n",
    "\n",
    "# 重新排列列顺序\n",
    "df = df.reindex(columns=new_columns)\n",
    "\n",
    "# 打印变换后的DataFrame\n",
    "print('变换后的DataFrame：')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NDVI_Planting', 'EVI_Planting', 'LSWI_Planting', 'GCVI_Planting', 'RVI_Planting', 'SAVI_Planting', 'WDRVI_Planting', 'Fpar_Planting', 'LAI_Planting', 'LE_Planting', 'LST_Day_Planting', 'LST_Night_Planting', 'SPI14d_Planting', 'SPI30d_Planting', 'SPI90d_Planting', 'EDDI14d_Planting', 'EDDI30d_Planting', 'EDDI90d_Planting', 'SPEI14d_Planting', 'SPEI30d_Planting', 'SPEI90d_Planting', 'PDSI_Planting', 'Palmer Z_Planting', 'Red_Planting', 'Nir_Planting', 'Blue_Planting', 'Green_Planting', 'Nir1_Planting', 'Swir1_Planting', 'Swir2_Planting', 'Temp_Planting', 'Shortwave_Planting', 'Longwave_Planting', 'ET_Planting', 'NDVI_Emerging', 'EVI_Emerging', 'LSWI_Emerging', 'GCVI_Emerging', 'RVI_Emerging', 'SAVI_Emerging', 'WDRVI_Emerging', 'Fpar_Emerging', 'LAI_Emerging', 'LE_Emerging', 'LST_Day_Emerging', 'LST_Night_Emerging', 'SPI14d_Emerging', 'SPI30d_Emerging', 'SPI90d_Emerging', 'EDDI14d_Emerging', 'EDDI30d_Emerging', 'EDDI90d_Emerging', 'SPEI14d_Emerging', 'SPEI30d_Emerging', 'SPEI90d_Emerging', 'PDSI_Emerging', 'Palmer Z_Emerging', 'Red_Emerging', 'Nir_Emerging', 'Blue_Emerging', 'Green_Emerging', 'Nir1_Emerging', 'Swir1_Emerging', 'Swir2_Emerging', 'Temp_Emerging', 'Shortwave_Emerging', 'Longwave_Emerging', 'ET_Emerging', 'NDVI_Blooming', 'EVI_Blooming', 'LSWI_Blooming', 'GCVI_Blooming', 'RVI_Blooming', 'SAVI_Blooming', 'WDRVI_Blooming', 'Fpar_Blooming', 'LAI_Blooming', 'LE_Blooming', 'LST_Day_Blooming', 'LST_Night_Blooming', 'SPI14d_Blooming', 'SPI30d_Blooming', 'SPI90d_Blooming', 'EDDI14d_Blooming', 'EDDI30d_Blooming', 'EDDI90d_Blooming', 'SPEI14d_Blooming', 'SPEI30d_Blooming', 'SPEI90d_Blooming', 'PDSI_Blooming', 'Palmer Z_Blooming', 'Red_Blooming', 'Nir_Blooming', 'Blue_Blooming', 'Green_Blooming', 'Nir1_Blooming', 'Swir1_Blooming', 'Swir2_Blooming', 'Temp_Blooming', 'Shortwave_Blooming', 'Longwave_Blooming', 'ET_Blooming', 'NDVI_Podding', 'EVI_Podding', 'LSWI_Podding', 'GCVI_Podding', 'RVI_Podding', 'SAVI_Podding', 'WDRVI_Podding', 'Fpar_Podding', 'LAI_Podding', 'LE_Podding', 'LST_Day_Podding', 'LST_Night_Podding', 'SPI14d_Podding', 'SPI30d_Podding', 'SPI90d_Podding', 'EDDI14d_Podding', 'EDDI30d_Podding', 'EDDI90d_Podding', 'SPEI14d_Podding', 'SPEI30d_Podding', 'SPEI90d_Podding', 'PDSI_Podding', 'Palmer Z_Podding', 'Red_Podding', 'Nir_Podding', 'Blue_Podding', 'Green_Podding', 'Nir1_Podding', 'Swir1_Podding', 'Swir2_Podding', 'Temp_Podding', 'Shortwave_Podding', 'Longwave_Podding', 'ET_Podding', 'NDVI_Dropping leaves', 'EVI_Dropping leaves', 'LSWI_Dropping leaves', 'GCVI_Dropping leaves', 'RVI_Dropping leaves', 'SAVI_Dropping leaves', 'WDRVI_Dropping leaves', 'Fpar_Dropping leaves', 'LAI_Dropping leaves', 'LE_Dropping leaves', 'LST_Day_Dropping leaves', 'LST_Night_Dropping leaves', 'SPI14d_Dropping leaves', 'SPI30d_Dropping leaves', 'SPI90d_Dropping leaves', 'EDDI14d_Dropping leaves', 'EDDI30d_Dropping leaves', 'EDDI90d_Dropping leaves', 'SPEI14d_Dropping leaves', 'SPEI30d_Dropping leaves', 'SPEI90d_Dropping leaves', 'PDSI_Dropping leaves', 'Palmer Z_Dropping leaves', 'Red_Dropping leaves', 'Nir_Dropping leaves', 'Blue_Dropping leaves', 'Green_Dropping leaves', 'Nir1_Dropping leaves', 'Swir1_Dropping leaves', 'Swir2_Dropping leaves', 'Temp_Dropping leaves', 'Shortwave_Dropping leaves', 'Longwave_Dropping leaves', 'ET_Dropping leaves', 'NDVI_Harvest', 'EVI_Harvest', 'LSWI_Harvest', 'GCVI_Harvest', 'RVI_Harvest', 'SAVI_Harvest', 'WDRVI_Harvest', 'Fpar_Harvest', 'LAI_Harvest', 'LE_Harvest', 'LST_Day_Harvest', 'LST_Night_Harvest', 'SPI14d_Harvest', 'SPI30d_Harvest', 'SPI90d_Harvest', 'EDDI14d_Harvest', 'EDDI30d_Harvest', 'EDDI90d_Harvest', 'SPEI14d_Harvest', 'SPEI30d_Harvest', 'SPEI90d_Harvest', 'PDSI_Harvest', 'Palmer Z_Harvest', 'Red_Harvest', 'Nir_Harvest', 'Blue_Harvest', 'Green_Harvest', 'Nir1_Harvest', 'Swir1_Harvest', 'Swir2_Harvest', 'Temp_Harvest', 'Shortwave_Harvest', 'Longwave_Harvest', 'ET_Harvest']\n"
     ]
    }
   ],
   "source": [
    "str_list = data.columns.tolist()\n",
    "sub_str_list = ['_Planting', '_Emerging', '_Blooming', '_Podding', '_Dropping leaves', '_Harvest']\n",
    "\n",
    "result_list = [s for s in str_list if any(sub in s for sub in sub_str_list)]\n",
    "# 按照顺序排序\n",
    "order = ['_Planting', '_Emerging', '_Blooming', '_Podding', '_Dropping leaves', '_Harvest']\n",
    "\n",
    "new_data = []\n",
    "for o in order:\n",
    "    for d in result_list:\n",
    "        if o in d:\n",
    "            new_data.append(d)\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-159a0e48da20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdynamic_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatic_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTMModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'keras'"
     ]
    }
   ],
   "source": [
    "class LSTMModel(tf.keras.Model):\n",
    "    def __init__(self, dynamic_shape, static_shape, lstm_units=16, dense_units=128, learning_rate=0.005):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm_units = lstm_units\n",
    "        self.dense_units = dense_units\n",
    "\n",
    "        self.dynamic_layer = tf.keras.layers.LSTM(units=lstm_units, return_sequences=False, input_shape=dynamic_shape)\n",
    "        self.static_layer = tf.keras.layers.InputLayer(input_shape=static_shape)\n",
    "        self.concat_layer = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self.dense_layer = tf.keras.layers.Dense(units=dense_units, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(units=1, activation='linear')\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        dynamic_inputs, static_inputs = inputs\n",
    "        x = self.dynamic_layer(dynamic_inputs)\n",
    "        x = self.concat_layer([x, static_inputs])\n",
    "        x = self.dense_layer(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "# Reshape the data\n",
    "X_train_dynamic = np.array(X_train_dynamic).reshape(10424,24,38)\n",
    "X_train_static = np.array(X_train_static).reshape(10424,19)\n",
    "y_train = np.array(y_train).reshape(10424,1)\n",
    "\n",
    "X_val_dynamic = np.array(X_val_dynamic).reshape(1840,24,38)\n",
    "X_val_static = np.array(X_val_static).reshape(1840,19)\n",
    "y_val = np.array(y_val).reshape(1840,1)\n",
    "\n",
    "X_test_dynamic = np.array(X_test_dynamic).reshape(775,24,38)\n",
    "X_test_static = np.array(X_test_static).reshape(775,19)\n",
    "y_test = np.array(y_test).reshape(775,1)\n",
    "\n",
    "# Create the model\n",
    "dynamic_shape = X_train_dynamic.shape[1:]\n",
    "static_shape = X_train_static.shape[1:]\n",
    "model = LSTMModel(dynamic_shape, static_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=model.optimizer, loss=model.loss)\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_dynamic, X_train_static], y_train, validation_data=([X_val_dynamic, X_val_static], y_val), epochs=50)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate([X_test_dynamic, X_test_static], y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_predictor(data, test_year=[2020], N=2, experiment = 'GDT', croptype = 'soybean', model = ['XGBoost'], verbose = 0,\n",
    "                    tune_model = False, model_parameter=None, result = False, result_dir = '', feature_importance = False,\n",
    "                    fea_eng = False,seed = 99,save_model=False, save_model_dir='', trend_n = 30,\n",
    "                    importance_dir = '', feature_list = None, n_features = 200, metric_list = []):\n",
    "    '''Yield predictor\n",
    "    Parameters:\n",
    "    \n",
    "    '''\n",
    "    # split train-validation and test data\n",
    "    # testset\n",
    "    data_test = data[data['Year'].isin(test_year)]\n",
    "    # tra-val set\n",
    "    data_train_val = data[data['Year'].isin([i for i in range(test_year[0]-N,test_year[0])])]\n",
    "    # split input and output\n",
    "    y_train_val = data_train_val['yield(t/ha)']\n",
    "    y_test = data_test['yield(t/ha)']\n",
    "    \n",
    "    if experiment == 'GDT': # GDT\n",
    "        # de-trend the yield using linear model\n",
    "        yield_mean = data_train_val['yield(t/ha)'].mean()\n",
    "        linear_m = linear_model.LinearRegression()\n",
    "        \n",
    "        data_train_val1 = data[data['Year'].isin([i for i in range(test_year[0]-trend_n,test_year[0])])]\n",
    "        X99 = np.array(data_train_val1['Year']).reshape(-1,1)\n",
    "        Y99 = np.array(data_train_val1['yield(t/ha)']).reshape(-1,1)\n",
    "\n",
    "        linear_m.fit(X99, Y99)\n",
    "        if linear_m.coef_[0,0]<0:\n",
    "            yield_mean = data_train_val['yield(t/ha)'].max()\n",
    "        print('The foluma of de-trend the yield: yield_new = yield - {:.3f}×year + {:.3f} - ({:.3f})'\n",
    "              .format(linear_m.coef_[0,0],yield_mean,linear_m.intercept_[0]))\n",
    "\n",
    "        # de-trend data_train_val yield \n",
    "        data1 = data_train_val.copy()\n",
    "        data1.loc[:,'yield(t/ha)_fix'] = data_train_val['yield(t/ha)'] - linear_m.coef_[0] * data_train_val['Year'] + yield_mean - linear_m.intercept_\n",
    "        data1 = data1.drop(['yield(t/ha)'], axis = 1).rename({'yield(t/ha)_fix':'yield(t/ha)'}, axis=1)\n",
    "        data_train_val = data1\n",
    "        # de-trend data_test yield \n",
    "        data1 = data_test.copy()\n",
    "        data1.loc[:,'yield(t/ha)_fix'] = data_test['yield(t/ha)'] - linear_m.coef_[0] * data_test['Year'] + yield_mean - linear_m.intercept_\n",
    "\n",
    "        data1 = data1.drop(['yield(t/ha)'], axis = 1).rename({'yield(t/ha)_fix':'yield(t/ha)'}, axis=1)\n",
    "        data_test = data1\n",
    "        y_train_val = data_train_val['yield(t/ha)']\n",
    "        y_test = data_test['yield(t/ha)']\n",
    "        \n",
    "        X_train_val = data_train_val.drop(['sta_con','State','yield(t/ha)','Year','Value','GDT_trend_yield'], axis=1)# \n",
    "        X_test = data_test.drop(['sta_con','State','yield(t/ha)','Year','Value','GDT_trend_yield'], axis=1) # 'Value'\n",
    "    else: None\n",
    " \n",
    "    # Scale numeric features\n",
    "    columns_to_scale = X_train_val.columns.tolist()\n",
    "    std_scaler = preprocessing.StandardScaler().fit(X_train_val[columns_to_scale])\n",
    "    X_train_val.loc[:,columns_to_scale] = std_scaler.transform(X_train_val[columns_to_scale])\n",
    "    X_test.loc[:,columns_to_scale] = std_scaler.transform(X_test[columns_to_scale])\n",
    "    \n",
    "    # 区分时间、物候、静态特征\n",
    "    feature_time = \n",
    "    feature_phology = \n",
    "    feature_static = \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def yield_predictor(data, data1980, test_year = [2020], feature_list = [], detrend = True, usehistoy = True, model = ['XGBoost'], verbose = 0, shape=False,valsize=0.15,\n",
    "                   tune_model = False, n_features = 500, model_parameter=None, learn_curve = False, result = False, feature_importance = False,\n",
    "                   corr = False, rfecv = False,n_curve = False,Phenology = False,time = False, fea_eng = False):\n",
    "    # Encoding categry feature\n",
    "    # count encoding\n",
    "    count_encoder = ce.count.CountEncoder(cols = ['_majority'],normalize = True).fit(data)\n",
    "    data = count_encoder.transform(data)\n",
    "#     # one-hot encoding\n",
    "#     onehot_cols = ['state_name','Planted_Drought','Emergrd_Drought','Blooming_Drought','Setting Pods_Drought','Dropping Leaves_Drought','Harvested_Drought']\n",
    "#     new_onehot_cols = pd.get_dummies(data[onehot_cols]).columns.tolist()\n",
    "#     data.loc[:,new_onehot_cols] = pd.get_dummies(data[onehot_cols])\n",
    "#     data = data.drop(onehot_cols,axis=1)\n",
    "    # label encoding\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data.loc[:,'irrigation'] = le.fit_transform(data['irrigation'])\n",
    "    data.loc[:,'state_name'] = le.fit_transform(data['state_name'])\n",
    "\n",
    "    # split train-validation and test data\n",
    "    data_test = data[data['year'].isin(test_year)]\n",
    "    data_train_val = data[np.logical_not(data['year'].isin(test_year))]\n",
    "    data1980 = data1980[np.logical_not(data1980['year'].isin(test_year))]\n",
    "\n",
    "    if detrend:\n",
    "        # de-trend the yield using linear model\n",
    "        yield_mean = data_train_val['yield'].mean()\n",
    "        linear_m = linear_model.LinearRegression()\n",
    "        if usehistoy:\n",
    "            X99 = np.array(data1980['year']).reshape(-1,1)\n",
    "            Y99 = np.array(data1980['yield']).reshape(-1,1)\n",
    "        else:\n",
    "            X99 = np.array(data_train_val['year']).reshape(-1,1)\n",
    "            Y99 = np.array(data_train_val['yield']).reshape(-1,1)\n",
    "        linear_m.fit(X99, Y99)\n",
    "    #         print('The foluma of de-trend the yield: yield_new = yield - {:.3f}×year + {:.3f} - ({:.3f})'\n",
    "    #               .format(linear_m.coef_[0,0],yield_mean,linear_m.intercept_[0]))\n",
    "        # de-trend data_train_val yield \n",
    "        data1 = data_train_val.copy()\n",
    "        data1.loc[:,'yield_fix'] = data_train_val['yield'] - linear_m.coef_[0] * data_train_val['year'] + yield_mean - linear_m.intercept_\n",
    "        data1 = data1.drop(['yield'], axis = 1).rename({'yield_fix':'yield'}, axis=1)\n",
    "        data_train_val = data1\n",
    "        # de-trend data_test yield \n",
    "        data1 = data_test.copy()\n",
    "        data1.loc[:,'yield_fix'] = data_test['yield'] - linear_m.coef_[0] * data_test['year'] + yield_mean - linear_m.intercept_\n",
    "        data1 = data1.drop(['yield'], axis = 1).rename({'yield_fix':'yield'}, axis=1)\n",
    "        data_test = data1\n",
    "    else: None\n",
    "\n",
    "    # split input and output\n",
    "    y_train_val = data_train_val['yield']\n",
    "    y_test = data_test['yield']\n",
    "    # print(data_train_val.head(5),y_train_val.head(5),data_test.head(5),y_test.head(5))\n",
    "    X_train_val = data_train_val.drop(['Value', 'county_name', 'year', 'yield', 'sta_con','Planted_Drought', 'Emergrd_Drought', 'Blooming_Drought', 'Setting Pods_Drought', 'Dropping Leaves_Drought', 'Harvested_Drought'],axis=1)\n",
    "    X_test = data_test.drop(['Value', 'county_name', 'year', 'yield', 'sta_con','Planted_Drought', 'Emergrd_Drought', 'Blooming_Drought', 'Setting Pods_Drought', 'Dropping Leaves_Drought', 'Harvested_Drought'],axis=1)\n",
    "    # print(X_train_val.columns.tolist())  \n",
    "    # Scale numeric features\n",
    "    # not-numeric features list\n",
    "    # print(data.columns.tolist())\n",
    "#     not_numeric_list = data.columns.tolist()[-63:]\n",
    "#     not_numeric_list.append('_majority')\n",
    "#     not_numeric_list = [ele for ele in not_numeric_list if ele not in ['caco3_kg_sq_m','cec_025','ec_025','max_om','paws_025','ph_025','sar',\n",
    "#                                                                        'sand_025','silt_025','clay_025']]\n",
    "    columns_to_scale = X_train_val.columns.tolist()#[item for item in X_train_val.columns.tolist() if item not in not_numeric_list]\n",
    "    std_scaler = preprocessing.MinMaxScaler().fit(X_train_val[columns_to_scale])# StandardScaler()\n",
    "    X_train_val.loc[:,columns_to_scale] = std_scaler.transform(X_train_val[columns_to_scale])\n",
    "    X_test.loc[:,columns_to_scale] = std_scaler.transform(X_test[columns_to_scale])\n",
    "    # print(len(X_train_val.columns.tolist()))  \n",
    "    #print(X_train_val.columns.tolist())\n",
    "    # dynamic data\n",
    "    all_features = X_train_val.columns.tolist()\n",
    "    dynamic_features = []\n",
    "    for i in range(9,33):\n",
    "        for index in ['NDVI','EVI','LSWI','GCVI','RVI','SAVI','WDRVI','Fpar','Lai','ET','LE','LST_Day_1km','LST_Night_1km',\n",
    "                      'spi14d','spi30d','spi90d','eddi14d','eddi30d','eddi90d','spei14d','spei30d','spei90d','pdsi','z',\n",
    "                      'sur_refl_b01','sur_refl_b02','sur_refl_b03','sur_refl_b04','sur_refl_b05','sur_refl_b06','sur_refl_b07',\n",
    "                      'total_precipitation','temperature','specific_humidity','pressure','shortwave_radiation','longwave_radiation','temp_differ']:#\n",
    "            dynamic_features.append(str(i)+'_'+index)\n",
    "    X_train_val_dynamic = X_train_val[dynamic_features]\n",
    "    X_test_dynamic = X_test[dynamic_features]\n",
    "    # print(len(X_test_dynamic.columns.tolist()))\n",
    "    # print(X_train_val.head(5))\n",
    "    # print(X_train_val_dynamic.head(5))\n",
    "    # static data\n",
    "    X_train_val_static = X_train_val[['PIC', '_majority', 'x', 'y','irrigation', 'caco3_kg_sq_m', 'cec_025', 'drainage_class_int', 'ec_025', 'i_class',\n",
    "                                     'n_class', 'max_om', 'paws_025', 'ph_025', 'sar', 'texture_025', 'sand_025', 'silt_025', 'clay_025']]\n",
    "    X_test_static = X_test[['PIC', '_majority', 'x', 'y','irrigation', 'caco3_kg_sq_m', 'cec_025', 'drainage_class_int', 'ec_025', 'i_class',\n",
    "                                     'n_class', 'max_om', 'paws_025', 'ph_025', 'sar', 'texture_025', 'sand_025', 'silt_025', 'clay_025']]\n",
    "    # print(len(X_test_static.columns.tolist()))\n",
    "    # print(X_train_val_static.head(5))\n",
    "    #print(X_train_val_dynamic.shape,X_train_val_static.shape,X_test_dynamic.shape,X_test_static.shape,y_train_val.shape, y_test.shape)\n",
    "    return X_train_val_dynamic,X_train_val_static,X_test_dynamic,X_test_static,y_train_val, y_test, yield_mean, linear_m.intercept_,linear_m.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_dynamic,X_train_val_static,X_test_dynamic,X_test_static,y_train_val, y_test, yield_mean, slope = yield_predictor(data, data1980, test_year = [2020],  \n",
    "                                                                                detrend=True, usehistoy=True, model=['XGBoost'],verbose=0,shape=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9_NDVI</th>\n",
       "      <th>9_EVI</th>\n",
       "      <th>9_LSWI</th>\n",
       "      <th>9_GCVI</th>\n",
       "      <th>9_RVI</th>\n",
       "      <th>9_SAVI</th>\n",
       "      <th>9_WDRVI</th>\n",
       "      <th>9_Fpar</th>\n",
       "      <th>9_Lai</th>\n",
       "      <th>9_ET</th>\n",
       "      <th>...</th>\n",
       "      <th>32_sur_refl_b05</th>\n",
       "      <th>32_sur_refl_b06</th>\n",
       "      <th>32_sur_refl_b07</th>\n",
       "      <th>32_total_precipitation</th>\n",
       "      <th>32_temperature</th>\n",
       "      <th>32_specific_humidity</th>\n",
       "      <th>32_pressure</th>\n",
       "      <th>32_shortwave_radiation</th>\n",
       "      <th>32_longwave_radiation</th>\n",
       "      <th>32_temp_differ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.389177</td>\n",
       "      <td>0.563946</td>\n",
       "      <td>0.145230</td>\n",
       "      <td>0.294910</td>\n",
       "      <td>0.135913</td>\n",
       "      <td>0.357349</td>\n",
       "      <td>0.194341</td>\n",
       "      <td>0.194636</td>\n",
       "      <td>0.091850</td>\n",
       "      <td>0.251403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511814</td>\n",
       "      <td>0.772097</td>\n",
       "      <td>0.936275</td>\n",
       "      <td>0.021905</td>\n",
       "      <td>0.683026</td>\n",
       "      <td>0.441730</td>\n",
       "      <td>0.789796</td>\n",
       "      <td>0.462955</td>\n",
       "      <td>0.560545</td>\n",
       "      <td>0.603385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.411093</td>\n",
       "      <td>0.566342</td>\n",
       "      <td>0.240310</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>0.148033</td>\n",
       "      <td>0.363829</td>\n",
       "      <td>0.210057</td>\n",
       "      <td>0.243893</td>\n",
       "      <td>0.107350</td>\n",
       "      <td>0.287634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389038</td>\n",
       "      <td>0.530964</td>\n",
       "      <td>0.576126</td>\n",
       "      <td>0.098688</td>\n",
       "      <td>0.633356</td>\n",
       "      <td>0.385406</td>\n",
       "      <td>0.781978</td>\n",
       "      <td>0.391996</td>\n",
       "      <td>0.521319</td>\n",
       "      <td>0.605832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.389002</td>\n",
       "      <td>0.566642</td>\n",
       "      <td>0.159589</td>\n",
       "      <td>0.290140</td>\n",
       "      <td>0.136104</td>\n",
       "      <td>0.360389</td>\n",
       "      <td>0.194498</td>\n",
       "      <td>0.203026</td>\n",
       "      <td>0.096911</td>\n",
       "      <td>0.244545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507156</td>\n",
       "      <td>0.763803</td>\n",
       "      <td>0.903471</td>\n",
       "      <td>0.032405</td>\n",
       "      <td>0.671446</td>\n",
       "      <td>0.415459</td>\n",
       "      <td>0.796052</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.543832</td>\n",
       "      <td>0.605457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.372884</td>\n",
       "      <td>0.580892</td>\n",
       "      <td>0.295023</td>\n",
       "      <td>0.270268</td>\n",
       "      <td>0.127754</td>\n",
       "      <td>0.382120</td>\n",
       "      <td>0.183498</td>\n",
       "      <td>0.200115</td>\n",
       "      <td>0.089118</td>\n",
       "      <td>0.231119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447717</td>\n",
       "      <td>0.658913</td>\n",
       "      <td>0.749776</td>\n",
       "      <td>0.226818</td>\n",
       "      <td>0.592486</td>\n",
       "      <td>0.357373</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.330696</td>\n",
       "      <td>0.474618</td>\n",
       "      <td>0.600239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.398608</td>\n",
       "      <td>0.555181</td>\n",
       "      <td>0.117805</td>\n",
       "      <td>0.301546</td>\n",
       "      <td>0.140503</td>\n",
       "      <td>0.343695</td>\n",
       "      <td>0.200482</td>\n",
       "      <td>0.162029</td>\n",
       "      <td>0.070559</td>\n",
       "      <td>0.302861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412129</td>\n",
       "      <td>0.662385</td>\n",
       "      <td>0.831894</td>\n",
       "      <td>0.027437</td>\n",
       "      <td>0.667326</td>\n",
       "      <td>0.427147</td>\n",
       "      <td>0.766433</td>\n",
       "      <td>0.421325</td>\n",
       "      <td>0.539829</td>\n",
       "      <td>0.625811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13769</th>\n",
       "      <td>0.421940</td>\n",
       "      <td>0.577851</td>\n",
       "      <td>0.240349</td>\n",
       "      <td>0.316163</td>\n",
       "      <td>0.152742</td>\n",
       "      <td>0.375160</td>\n",
       "      <td>0.216572</td>\n",
       "      <td>0.277221</td>\n",
       "      <td>0.119788</td>\n",
       "      <td>0.413628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419818</td>\n",
       "      <td>0.500186</td>\n",
       "      <td>0.454273</td>\n",
       "      <td>0.106923</td>\n",
       "      <td>0.620622</td>\n",
       "      <td>0.449667</td>\n",
       "      <td>0.787565</td>\n",
       "      <td>0.313826</td>\n",
       "      <td>0.548886</td>\n",
       "      <td>0.549373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13770</th>\n",
       "      <td>0.453818</td>\n",
       "      <td>0.594876</td>\n",
       "      <td>0.294579</td>\n",
       "      <td>0.334383</td>\n",
       "      <td>0.170511</td>\n",
       "      <td>0.400367</td>\n",
       "      <td>0.239573</td>\n",
       "      <td>0.329443</td>\n",
       "      <td>0.152558</td>\n",
       "      <td>0.416827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449014</td>\n",
       "      <td>0.515810</td>\n",
       "      <td>0.476249</td>\n",
       "      <td>0.118764</td>\n",
       "      <td>0.620711</td>\n",
       "      <td>0.447980</td>\n",
       "      <td>0.791241</td>\n",
       "      <td>0.325473</td>\n",
       "      <td>0.552001</td>\n",
       "      <td>0.587888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13771</th>\n",
       "      <td>0.409460</td>\n",
       "      <td>0.580115</td>\n",
       "      <td>0.274076</td>\n",
       "      <td>0.316685</td>\n",
       "      <td>0.145916</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.207684</td>\n",
       "      <td>0.257410</td>\n",
       "      <td>0.114728</td>\n",
       "      <td>0.422029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530324</td>\n",
       "      <td>0.604586</td>\n",
       "      <td>0.597367</td>\n",
       "      <td>0.146142</td>\n",
       "      <td>0.586232</td>\n",
       "      <td>0.415485</td>\n",
       "      <td>0.740834</td>\n",
       "      <td>0.261313</td>\n",
       "      <td>0.519156</td>\n",
       "      <td>0.603902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13772</th>\n",
       "      <td>0.483582</td>\n",
       "      <td>0.627266</td>\n",
       "      <td>0.327189</td>\n",
       "      <td>0.381001</td>\n",
       "      <td>0.187948</td>\n",
       "      <td>0.454391</td>\n",
       "      <td>0.261895</td>\n",
       "      <td>0.375952</td>\n",
       "      <td>0.181427</td>\n",
       "      <td>0.453863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479881</td>\n",
       "      <td>0.336928</td>\n",
       "      <td>0.295148</td>\n",
       "      <td>0.219759</td>\n",
       "      <td>0.576586</td>\n",
       "      <td>0.404974</td>\n",
       "      <td>0.731591</td>\n",
       "      <td>0.273936</td>\n",
       "      <td>0.518663</td>\n",
       "      <td>0.423990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13773</th>\n",
       "      <td>0.544525</td>\n",
       "      <td>0.639203</td>\n",
       "      <td>0.355501</td>\n",
       "      <td>0.446320</td>\n",
       "      <td>0.227323</td>\n",
       "      <td>0.479605</td>\n",
       "      <td>0.311067</td>\n",
       "      <td>0.408443</td>\n",
       "      <td>0.189124</td>\n",
       "      <td>0.434646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422265</td>\n",
       "      <td>0.393218</td>\n",
       "      <td>0.333814</td>\n",
       "      <td>0.177026</td>\n",
       "      <td>0.571127</td>\n",
       "      <td>0.398255</td>\n",
       "      <td>0.739918</td>\n",
       "      <td>0.250874</td>\n",
       "      <td>0.510442</td>\n",
       "      <td>0.524491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12264 rows × 912 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         9_NDVI     9_EVI    9_LSWI    9_GCVI     9_RVI    9_SAVI   9_WDRVI  \\\n",
       "0      0.389177  0.563946  0.145230  0.294910  0.135913  0.357349  0.194341   \n",
       "1      0.411093  0.566342  0.240310  0.312766  0.148033  0.363829  0.210057   \n",
       "2      0.389002  0.566642  0.159589  0.290140  0.136104  0.360389  0.194498   \n",
       "3      0.372884  0.580892  0.295023  0.270268  0.127754  0.382120  0.183498   \n",
       "4      0.398608  0.555181  0.117805  0.301546  0.140503  0.343695  0.200482   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "13769  0.421940  0.577851  0.240349  0.316163  0.152742  0.375160  0.216572   \n",
       "13770  0.453818  0.594876  0.294579  0.334383  0.170511  0.400367  0.239573   \n",
       "13771  0.409460  0.580115  0.274076  0.316685  0.145916  0.378057  0.207684   \n",
       "13772  0.483582  0.627266  0.327189  0.381001  0.187948  0.454391  0.261895   \n",
       "13773  0.544525  0.639203  0.355501  0.446320  0.227323  0.479605  0.311067   \n",
       "\n",
       "         9_Fpar     9_Lai      9_ET  ...  32_sur_refl_b05  32_sur_refl_b06  \\\n",
       "0      0.194636  0.091850  0.251403  ...         0.511814         0.772097   \n",
       "1      0.243893  0.107350  0.287634  ...         0.389038         0.530964   \n",
       "2      0.203026  0.096911  0.244545  ...         0.507156         0.763803   \n",
       "3      0.200115  0.089118  0.231119  ...         0.447717         0.658913   \n",
       "4      0.162029  0.070559  0.302861  ...         0.412129         0.662385   \n",
       "...         ...       ...       ...  ...              ...              ...   \n",
       "13769  0.277221  0.119788  0.413628  ...         0.419818         0.500186   \n",
       "13770  0.329443  0.152558  0.416827  ...         0.449014         0.515810   \n",
       "13771  0.257410  0.114728  0.422029  ...         0.530324         0.604586   \n",
       "13772  0.375952  0.181427  0.453863  ...         0.479881         0.336928   \n",
       "13773  0.408443  0.189124  0.434646  ...         0.422265         0.393218   \n",
       "\n",
       "       32_sur_refl_b07  32_total_precipitation  32_temperature  \\\n",
       "0             0.936275                0.021905        0.683026   \n",
       "1             0.576126                0.098688        0.633356   \n",
       "2             0.903471                0.032405        0.671446   \n",
       "3             0.749776                0.226818        0.592486   \n",
       "4             0.831894                0.027437        0.667326   \n",
       "...                ...                     ...             ...   \n",
       "13769         0.454273                0.106923        0.620622   \n",
       "13770         0.476249                0.118764        0.620711   \n",
       "13771         0.597367                0.146142        0.586232   \n",
       "13772         0.295148                0.219759        0.576586   \n",
       "13773         0.333814                0.177026        0.571127   \n",
       "\n",
       "       32_specific_humidity  32_pressure  32_shortwave_radiation  \\\n",
       "0                  0.441730     0.789796                0.462955   \n",
       "1                  0.385406     0.781978                0.391996   \n",
       "2                  0.415459     0.796052                0.470395   \n",
       "3                  0.357373     0.726248                0.330696   \n",
       "4                  0.427147     0.766433                0.421325   \n",
       "...                     ...          ...                     ...   \n",
       "13769              0.449667     0.787565                0.313826   \n",
       "13770              0.447980     0.791241                0.325473   \n",
       "13771              0.415485     0.740834                0.261313   \n",
       "13772              0.404974     0.731591                0.273936   \n",
       "13773              0.398255     0.739918                0.250874   \n",
       "\n",
       "       32_longwave_radiation  32_temp_differ  \n",
       "0                   0.560545        0.603385  \n",
       "1                   0.521319        0.605832  \n",
       "2                   0.543832        0.605457  \n",
       "3                   0.474618        0.600239  \n",
       "4                   0.539829        0.625811  \n",
       "...                      ...             ...  \n",
       "13769               0.548886        0.549373  \n",
       "13770               0.552001        0.587888  \n",
       "13771               0.519156        0.603902  \n",
       "13772               0.518663        0.423990  \n",
       "13773               0.510442        0.524491  \n",
       "\n",
       "[12264 rows x 912 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13774    59.79863\n",
       "13776    57.29863\n",
       "13777    54.39863\n",
       "13778    59.79863\n",
       "13779    54.49863\n",
       "           ...   \n",
       "14634    45.69863\n",
       "14635    48.59863\n",
       "14636    49.79863\n",
       "14637    48.49863\n",
       "14638    42.59863\n",
       "Name: yield, Length: 775, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10424, 912) (10424, 19) (10424,) (1840, 912) (1840, 19) (1840,) (775, 912) (775, 19) (775,)\n"
     ]
    }
   ],
   "source": [
    "X_train_dynamic, X_val_dynamic, y_train, y_val = train_test_split(X_train_val_dynamic,y_train_val, test_size=0.15, random_state= 99)\n",
    "X_train_static, X_val_static, y_train, y_val = train_test_split(X_train_val_static,y_train_val, test_size=0.15, random_state= 99)\n",
    "print(X_train_dynamic.shape,X_train_static.shape,y_train.shape,X_val_dynamic.shape,X_val_static.shape, y_val.shape,X_test_dynamic.shape,X_test_static.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_MAE: 2.8577\n",
      "Test_MSE: 14.0479\n",
      "Test_RMSE: 3.7480\n",
      "Test_R2: 0.8496\n",
      "----\n",
      "None\n",
      "Test_MAE: 6.4138\n",
      "Test_MSE: 61.5298\n",
      "Test_RMSE: 7.8441\n",
      "Test_R2: 0.2766\n",
      "----\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "est = linear_model.LinearRegression()\n",
    "est.fit(np.concatenate((X_train_dynamic,X_train_static),axis = -1), y_train)\n",
    "print(test_metrics(y_val, est.predict(np.concatenate((X_val_dynamic,X_val_static),axis = -1))))\n",
    "print(test_metrics(y_test, est.predict(np.concatenate((X_test_dynamic,X_test_static),axis = -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm\n",
    "training_x_dynamic = np.array(X_train_dynamic).reshape(10424,24,38)\n",
    "training_x_static = np.array(X_train_static).reshape(10424,19)\n",
    "training_y = np.array(y_train).reshape(10424,1)\n",
    "\n",
    "val_x_dynamic = np.array(X_val_dynamic).reshape(1840,24,38)\n",
    "val_x_static = np.array(X_val_static).reshape(1840,19)\n",
    "val_y = np.array(y_val).reshape(1840,1)\n",
    "\n",
    "test_x_dynamic = np.array(X_test_dynamic).reshape(775,24,38)\n",
    "test_x_static = np.array(X_test_static).reshape(775,19)\n",
    "test_y = np.array(y_test).reshape(775,1)\n",
    "\n",
    "\n",
    "unit = 16\n",
    "inp1 = tf.keras.layers.Input(shape=(24,38))\n",
    "inp2 = tf.keras.layers.Input(shape=(19,))\n",
    "LSTM1 = tf.keras.layers.LSTM(units=unit, return_sequences=False)(inp1)\n",
    "# LSTM2 = tf.keras.layers.LSTM(units=unit, return_sequences=True)(LSTM1)\n",
    "# LSTM3 = tf.keras.layers.LSTM(units=unit, return_sequences=True)(LSTM2)\n",
    "# LSTM4 = tf.keras.layers.LSTM(units=32, return_sequences=False)(LSTM1)\n",
    "comb = tf.keras.layers.Concatenate(axis=-1)([LSTM1, inp2])\n",
    "DENSE_1 = tf.keras.layers.Dense(units=128, activation='relu')(comb)\n",
    "outp = tf.keras.layers.Dense(units=1, activation='linear')(DENSE_1)\n",
    "model = tf.keras.Model([inp1, inp2], outp)\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.005), loss=tf.keras.losses.MeanSquaredError())# 'mean_absolute_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnn\n",
    "# (10424, 912) (10424, 19) (1840, 912) (1840, 19) (12264,) (1840,)\n",
    "#(9415, 912) (9415, 19) (9415,) (1662, 912) (1662, 19) (1662,) (1962, 912) (1962, 19) (1962,)\n",
    "training_x_dynamic = np.array(X_train_dynamic).reshape(X_train_dynamic.shape[0],912)#10424\n",
    "training_x_static = np.array(X_train_static).reshape(X_train_dynamic.shape[0],19)\n",
    "training_y = np.array(y_train).reshape(X_train_dynamic.shape[0],1)\n",
    "\n",
    "val_x_dynamic = np.array(X_val_dynamic).reshape(X_val_dynamic.shape[0],912)#1840\n",
    "val_x_static = np.array(X_val_static).reshape(X_val_dynamic.shape[0],19)\n",
    "val_y = np.array(y_val).reshape(X_val_dynamic.shape[0],1)\n",
    "\n",
    "test_x_dynamic = np.array(X_test_dynamic).reshape(X_test_dynamic.shape[0],912)# \n",
    "test_x_static = np.array(X_test_static).reshape(X_test_dynamic.shape[0],19)\n",
    "test_y = np.array(y_test).reshape(X_test_dynamic.shape[0],1)\n",
    "\n",
    "func_act = 'relu'\n",
    "inp1 = tf.keras.layers.Input(shape=(912))\n",
    "inp2 = tf.keras.layers.Input(shape=(19))\n",
    "inp = tf.keras.layers.Concatenate(axis=-1)([inp1, inp2])\n",
    "DENSE_0 = tf.keras.layers.Dense(units=512, activation=func_act)(inp)# ,kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "# DENSE_01 = tf.keras.layers.Dropout(0.2)(DENSE_0)\n",
    "DENSE_1 = tf.keras.layers.Dense(units=256, activation=func_act)(DENSE_0)#,kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "DENSE_2 = tf.keras.layers.Dense(units=128, activation=func_act)(DENSE_1)\n",
    "DENSE_3 = tf.keras.layers.Dense(units=64, activation=func_act)(DENSE_2)\n",
    "DENSE_4 = tf.keras.layers.Dense(units=32, activation=func_act)(DENSE_3)\n",
    "DENSE_5 = tf.keras.layers.Dense(units=16, activation=func_act)(DENSE_4)\n",
    "DENSE_6 = tf.keras.layers.Dense(units=8, activation=func_act)(DENSE_5)\n",
    "outp = tf.keras.layers.Dense(units=1, activation='linear')(DENSE_6)\n",
    "model = tf.keras.Model([inp1, inp2], outp)\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.01), loss=tf.keras.losses.MeanSquaredError())# 'mean_absolute_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for all data: -1.0849\n",
      "Average R2 score for subsets: -1.3088\n",
      "Are the results equal? False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 生成模拟数据\n",
    "y_true = np.random.rand(100)\n",
    "y_pred = np.random.rand(100)\n",
    "\n",
    "# 将数据分为三个子集\n",
    "n = len(y_true) // 3\n",
    "y_true_subsets = [y_true[i:i+n] for i in range(0, len(y_true), n)]\n",
    "y_pred_subsets = [y_pred[i:i+n] for i in range(0, len(y_pred), n)]\n",
    "\n",
    "# 分别计算每个子集的R2\n",
    "r2_subsets = [r2_score(y_true_subsets[i], y_pred_subsets[i]) for i in range(3)]\n",
    "\n",
    "# 计算所有数据的总体R2\n",
    "r2_total = r2_score(y_true, y_pred)\n",
    "\n",
    "# 将三个子集的R2求平均值\n",
    "r2_subsets_avg = np.mean(r2_subsets)\n",
    "\n",
    "# 比较结果\n",
    "print(\"R2 score for all data: %.4f\" % r2_total)\n",
    "print(\"Average R2 score for subsets: %.4f\" % r2_subsets_avg)\n",
    "print(\"Are the results equal? %s\" % np.isclose(r2_total, r2_subsets_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_MAE: 155015.8806\n",
      "Test_MSE: 24179252143.2908\n",
      "Test_RMSE: 155496.7914\n",
      "Test_R2: -284264516.9668\n",
      "----\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "est = linear_model.LinearRegression()\n",
    "est.fit(np.concatenate((training_x_dynamic,training_x_static),axis = -1), training_y)\n",
    "print(test_metrics(test_y, est.predict(np.concatenate((test_x_dynamic,test_x_static),axis = -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 140.4014 - val_loss: 69.6001\n",
      "Epoch 2/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 62.0067 - val_loss: 56.0873\n",
      "Epoch 3/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 55.3468 - val_loss: 57.9348\n",
      "Epoch 4/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 53.1197 - val_loss: 49.2860\n",
      "Epoch 5/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 50.2567 - val_loss: 47.8326\n",
      "Epoch 6/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 47.9625 - val_loss: 45.9058\n",
      "Epoch 7/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 44.8841 - val_loss: 45.0421\n",
      "Epoch 8/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 43.3754 - val_loss: 40.8586\n",
      "Epoch 9/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 40.4264 - val_loss: 38.9600\n",
      "Epoch 10/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 38.3968 - val_loss: 48.6238\n",
      "Epoch 11/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 37.6321 - val_loss: 35.1961\n",
      "Epoch 12/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 34.9413 - val_loss: 37.0997\n",
      "Epoch 13/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 34.0326 - val_loss: 32.9300\n",
      "Epoch 14/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 32.9464 - val_loss: 32.8410\n",
      "Epoch 15/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 32.0851 - val_loss: 30.9291\n",
      "Epoch 16/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 30.8176 - val_loss: 35.3565\n",
      "Epoch 17/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 30.4939 - val_loss: 28.8592\n",
      "Epoch 18/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 29.2873 - val_loss: 31.4974\n",
      "Epoch 19/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 28.5610 - val_loss: 28.5027\n",
      "Epoch 20/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 27.5243 - val_loss: 26.9302\n",
      "Epoch 21/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 26.7412 - val_loss: 26.5370\n",
      "Epoch 22/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 26.8816 - val_loss: 37.9865\n",
      "Epoch 23/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 25.9870 - val_loss: 33.3867\n",
      "Epoch 24/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 26.6001 - val_loss: 26.0488\n",
      "Epoch 25/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 25.4487 - val_loss: 24.9872\n",
      "Epoch 26/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 25.5622 - val_loss: 26.6296\n",
      "Epoch 27/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 24.8030 - val_loss: 27.5618\n",
      "Epoch 28/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 25.4605 - val_loss: 28.6344\n",
      "Epoch 29/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 24.1451 - val_loss: 35.2979\n",
      "Epoch 30/150\n",
      "515/522 [============================>.] - ETA: 0s - loss: 24.0648\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 24.0413 - val_loss: 25.9036\n",
      "Epoch 31/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 22.6112 - val_loss: 24.1651\n",
      "Epoch 32/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 22.1060 - val_loss: 25.5074\n",
      "Epoch 33/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 21.6447 - val_loss: 25.2757\n",
      "Epoch 34/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 21.7659 - val_loss: 23.4739\n",
      "Epoch 35/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 21.4322 - val_loss: 23.3773\n",
      "Epoch 36/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 21.2665 - val_loss: 23.5535\n",
      "Epoch 37/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 21.1442 - val_loss: 24.0685\n",
      "Epoch 38/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 20.9653 - val_loss: 23.4874\n",
      "Epoch 39/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 21.0440 - val_loss: 23.5400\n",
      "Epoch 40/150\n",
      "520/522 [============================>.] - ETA: 0s - loss: 20.8372\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 20.8192 - val_loss: 23.8588\n",
      "Epoch 41/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 20.0129 - val_loss: 22.9979\n",
      "Epoch 42/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 19.7922 - val_loss: 22.3137\n",
      "Epoch 43/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 19.7768 - val_loss: 23.0455\n",
      "Epoch 44/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 19.5882 - val_loss: 22.1458\n",
      "Epoch 45/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 19.3595 - val_loss: 23.6324\n",
      "Epoch 46/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 19.2213 - val_loss: 22.2310\n",
      "Epoch 47/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 18.8410 - val_loss: 21.6733\n",
      "Epoch 48/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 18.5896 - val_loss: 21.2830\n",
      "Epoch 49/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 18.4843 - val_loss: 21.8968\n",
      "Epoch 50/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 18.2233 - val_loss: 20.9528\n",
      "Epoch 51/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 17.8517 - val_loss: 21.3612\n",
      "Epoch 52/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 17.6380 - val_loss: 20.4199\n",
      "Epoch 53/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 17.6299 - val_loss: 19.7340\n",
      "Epoch 54/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 17.3722 - val_loss: 19.7590\n",
      "Epoch 55/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 17.1357 - val_loss: 20.1294\n",
      "Epoch 56/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 17.0031 - val_loss: 20.4821\n",
      "Epoch 57/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 16.9225 - val_loss: 19.1997\n",
      "Epoch 58/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 16.6428 - val_loss: 19.2987\n",
      "Epoch 59/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 16.3391 - val_loss: 19.5247\n",
      "Epoch 60/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 16.2945 - val_loss: 18.5608\n",
      "Epoch 61/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 16.2183 - val_loss: 18.2259\n",
      "Epoch 62/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.9730 - val_loss: 18.1737\n",
      "Epoch 63/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.8055 - val_loss: 17.3648\n",
      "Epoch 64/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.8264 - val_loss: 18.4795\n",
      "Epoch 65/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.9131 - val_loss: 17.5118\n",
      "Epoch 66/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.4052 - val_loss: 17.2417\n",
      "Epoch 67/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.4319 - val_loss: 17.0257\n",
      "Epoch 68/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.2867 - val_loss: 19.0571\n",
      "Epoch 69/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.1808 - val_loss: 18.0578\n",
      "Epoch 70/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 15.0376 - val_loss: 17.0046\n",
      "Epoch 71/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.9502 - val_loss: 16.8207\n",
      "Epoch 72/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.9995 - val_loss: 16.8387\n",
      "Epoch 73/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.7008 - val_loss: 17.1773\n",
      "Epoch 74/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.7005 - val_loss: 16.4420\n",
      "Epoch 75/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.6136 - val_loss: 16.7300\n",
      "Epoch 76/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.3707 - val_loss: 16.1727\n",
      "Epoch 77/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.2509 - val_loss: 16.4679\n",
      "Epoch 78/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.3468 - val_loss: 16.1277\n",
      "Epoch 79/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.3658 - val_loss: 15.6988\n",
      "Epoch 80/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.0636 - val_loss: 17.9981\n",
      "Epoch 81/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 14.0514 - val_loss: 15.7919\n",
      "Epoch 82/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.8121 - val_loss: 16.3061\n",
      "Epoch 83/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.8266 - val_loss: 15.4203\n",
      "Epoch 84/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.9024 - val_loss: 16.2743\n",
      "Epoch 85/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.7242 - val_loss: 15.6013\n",
      "Epoch 86/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.6798 - val_loss: 16.3346\n",
      "Epoch 87/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.5526 - val_loss: 17.0000\n",
      "Epoch 88/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.4163 - val_loss: 15.3355\n",
      "Epoch 89/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.2297 - val_loss: 15.2000\n",
      "Epoch 90/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.2386 - val_loss: 15.2932\n",
      "Epoch 91/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.2390 - val_loss: 15.5968\n",
      "Epoch 92/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.1971 - val_loss: 15.4088\n",
      "Epoch 93/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.0334 - val_loss: 15.7557\n",
      "Epoch 94/150\n",
      "516/522 [============================>.] - ETA: 0s - loss: 13.0549\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 13.0964 - val_loss: 15.3607\n",
      "Epoch 95/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.4299 - val_loss: 15.1348\n",
      "Epoch 96/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.3934 - val_loss: 15.2754\n",
      "Epoch 97/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.4056 - val_loss: 14.8368\n",
      "Epoch 98/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.3095 - val_loss: 14.9082\n",
      "Epoch 99/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.2518 - val_loss: 14.4636\n",
      "Epoch 100/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.3218 - val_loss: 14.5353\n",
      "Epoch 101/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.1964 - val_loss: 14.9926\n",
      "Epoch 102/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.1731 - val_loss: 14.5236\n",
      "Epoch 103/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.1659 - val_loss: 14.8159\n",
      "Epoch 104/150\n",
      "519/522 [============================>.] - ETA: 0s - loss: 12.1254\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 12.1487 - val_loss: 15.5100\n",
      "Epoch 105/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.7893 - val_loss: 14.6746\n",
      "Epoch 106/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.7407 - val_loss: 14.4310\n",
      "Epoch 107/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.7371 - val_loss: 14.2682\n",
      "Epoch 108/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.7117 - val_loss: 14.5011\n",
      "Epoch 109/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.6566 - val_loss: 14.6744\n",
      "Epoch 110/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.6699 - val_loss: 14.2673\n",
      "Epoch 111/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.6255 - val_loss: 14.2534\n",
      "Epoch 112/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.7013 - val_loss: 14.3362\n",
      "Epoch 113/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.6264 - val_loss: 14.3184\n",
      "Epoch 114/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.6147 - val_loss: 14.6260\n",
      "Epoch 115/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.5502 - val_loss: 14.1646\n",
      "Epoch 116/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.5878 - val_loss: 14.2173\n",
      "Epoch 117/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.5668 - val_loss: 14.2335\n",
      "Epoch 118/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.4825 - val_loss: 14.1692\n",
      "Epoch 119/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.5121 - val_loss: 14.3051\n",
      "Epoch 120/150\n",
      "517/522 [============================>.] - ETA: 0s - loss: 11.4643\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.4550 - val_loss: 14.3926\n",
      "Epoch 121/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.3340 - val_loss: 14.4543\n",
      "Epoch 122/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.3007 - val_loss: 14.2974\n",
      "Epoch 123/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.3045 - val_loss: 14.1720\n",
      "Epoch 124/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2839 - val_loss: 14.2768\n",
      "Epoch 125/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2691 - val_loss: 14.1298\n",
      "Epoch 126/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2643 - val_loss: 14.1956\n",
      "Epoch 127/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2384 - val_loss: 14.1938\n",
      "Epoch 128/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2542 - val_loss: 14.1035\n",
      "Epoch 129/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2560 - val_loss: 14.1961\n",
      "Epoch 130/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2177 - val_loss: 14.1599\n",
      "Epoch 131/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2098 - val_loss: 14.1331\n",
      "Epoch 132/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.2262 - val_loss: 14.2306\n",
      "Epoch 133/150\n",
      "512/522 [============================>.] - ETA: 0s - loss: 11.2104\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.1896 - val_loss: 14.1884\n",
      "Epoch 134/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.1163 - val_loss: 14.1048\n",
      "Epoch 135/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.1028 - val_loss: 14.0548\n",
      "Epoch 136/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0971 - val_loss: 14.1544\n",
      "Epoch 137/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.1057 - val_loss: 14.0868\n",
      "Epoch 138/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0990 - val_loss: 14.0545\n",
      "Epoch 139/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0971 - val_loss: 14.1144\n",
      "Epoch 140/150\n",
      "516/522 [============================>.] - ETA: 0s - loss: 11.1022\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0733 - val_loss: 14.0760\n",
      "Epoch 141/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0309 - val_loss: 14.1694\n",
      "Epoch 142/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0386 - val_loss: 14.1453\n",
      "Epoch 143/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0346 - val_loss: 14.0854\n",
      "Epoch 144/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0344 - val_loss: 14.0653\n",
      "Epoch 145/150\n",
      "515/522 [============================>.] - ETA: 0s - loss: 11.0641\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0280 - val_loss: 14.0675\n",
      "Epoch 146/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0008 - val_loss: 14.0681\n",
      "Epoch 147/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 10.9976 - val_loss: 14.0564\n",
      "Epoch 148/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0020 - val_loss: 14.0498\n",
      "Epoch 149/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 11.0012 - val_loss: 14.0666\n",
      "Epoch 150/150\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 10.9993 - val_loss: 14.0608\n"
     ]
    }
   ],
   "source": [
    "# Stop training when a monitored metric has stopped improving.监控val_loss,当连续10个epoches，val_loss的变化没有大于0.0001，视为无改进，停止训练，输出进度条\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1)\n",
    "# Reduce learning rate when a metric has stopped improving.监控val_loss,当连续10个epoches，val_loss的变化没有大于0.001，视为无改进，学习率减半，输出进度条\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', min_delta=1e-3, factor=0.5, patience=5, verbose=1)\n",
    "start = time.time()\n",
    "history = model.fit([training_x_dynamic,training_x_static], training_y, shuffle=True, epochs=150, callbacks=[early_stop, reduce_lr], #\n",
    "                    validation_data=([val_x_dynamic,val_x_static], val_y), verbose=1, batch_size=20)# val_x_dynamic,val_x_static], val_y\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "code_folding": [
     1,
     7,
     56,
     71
    ]
   },
   "outputs": [],
   "source": [
    "def tune_param(testyear = [2020],MODEL=['LSTM'],verbose=0,shape=False,UNIT=[32],LR = [0.001],BATCH_SIZE=[10],Result=False,seeds=9):\n",
    "    X_train_val_dynamic,X_train_val_static,X_test_dynamic,X_test_static,y_train_val, y_test,yield_mean, b, slope= yield_predictor(data, data1980, test_year = testyear,  \n",
    "                                                                                detrend=True, usehistoy=True)\n",
    "    X_train_dynamic, X_val_dynamic, y_train, y_val = train_test_split(X_train_val_dynamic,y_train_val, test_size=0.15, random_state=seeds)\n",
    "    X_train_static, X_val_static, y_train, y_val = train_test_split(X_train_val_static,y_train_val, test_size=0.15, random_state= seeds)\n",
    "    trainshape,valshape,testshape = X_train_dynamic.shape[0],X_val_dynamic.shape[0],X_test_dynamic.shape[0]\n",
    "    print(testyear)\n",
    "    if shape:\n",
    "        print('train:',X_train_dynamic.shape,X_train_static.shape,y_train.shape,\n",
    "              '\\n val:',X_val_dynamic.shape,X_val_static.shape, y_val.shape,\n",
    "              '\\n test:',X_test_dynamic.shape,X_test_static.shape,y_test.shape)\n",
    "    if MODEL == ['LSTM']:\n",
    "        # lstm\n",
    "        training_x_dynamic = np.array(X_train_dynamic).reshape(trainshape,24,38)\n",
    "        training_x_static = np.array(X_train_static).reshape(trainshape,19)\n",
    "        training_y = np.array(y_train).reshape(trainshape,1)\n",
    "\n",
    "        val_x_dynamic = np.array(X_val_dynamic).reshape(valshape,24,38)\n",
    "        val_x_static = np.array(X_val_static).reshape(valshape,19)\n",
    "        val_y = np.array(y_val).reshape(valshape,1)\n",
    "\n",
    "        test_x_dynamic = np.array(X_test_dynamic).reshape(testshape,24,38)\n",
    "        test_x_static = np.array(X_test_static).reshape(testshape,19)\n",
    "        test_y = np.array(y_test).reshape(testshape,1)\n",
    "        ratio = str(trainshape)+':'+str(valshape)+':'+str(testshape)\n",
    "\n",
    "\n",
    "        for i in UNIT:\n",
    "            for j in LR:\n",
    "                for k in BATCH_SIZE:\n",
    "                    print('UNIT=',i,\",LR=\",j,',BATCH_SIZE=',k)\n",
    "                    unit = i\n",
    "                    inp1 = tf.keras.layers.Input(shape=(24,38))\n",
    "                    inp2 = tf.keras.layers.Input(shape=(19,))\n",
    "                    LSTM1 = tf.keras.layers.LSTM(units=unit, return_sequences=True)(inp1)\n",
    "                    LSTM2 = tf.keras.layers.LSTM(units=unit, return_sequences=False)(LSTM1)\n",
    "                    # LSTM2 = tf.keras.layers.LSTM(units=unit, return_sequences=True)(LSTM1)\n",
    "                    # LSTM3 = tf.keras.layers.LSTM(units=unit, return_sequences=True)(LSTM2)\n",
    "                    # LSTM4 = tf.keras.layers.LSTM(units=32, return_sequences=False)(LSTM1)\n",
    "                    comb = tf.keras.layers.Concatenate(axis=-1)([LSTM2, inp2])\n",
    "                    DENSE_1 = tf.keras.layers.Dense(units=unit, activation='relu')(comb)\n",
    "                    outp = tf.keras.layers.Dense(units=1, activation='linear')(DENSE_1)\n",
    "                    model = tf.keras.Model([inp1, inp2], outp)\n",
    "                    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=j), loss=tf.keras.losses.MeanSquaredError())# MeanAbsoluteError\n",
    "                    early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1)\n",
    "                    reduce_lr = ReduceLROnPlateau(monitor='val_loss', min_delta=1e-3, factor=0.5, patience=5, verbose=1)# \n",
    "#                     start = time.time()\n",
    "#                     history = model.fit([training_x_dynamic,training_x_static], training_y, shuffle=True, epochs=100, callbacks=[early_stop, reduce_lr], #\n",
    "#                                         validation_data=([val_x_dynamic,val_x_static], val_y), verbose=1, batch_size=k)# val_x_dynamic,val_x_static], val_y\n",
    "#                     end = time.time()\n",
    "                    if verbose == 0:\n",
    "                        raw,result = model_lineresult(model,MODEL,ratio,testyear,[training_x_dynamic,training_x_static], [val_x_dynamic,val_x_static], \n",
    "                                             [test_x_dynamic,test_x_static], training_y, val_y, test_y, k,early_stop, reduce_lr)\n",
    "                    else :\n",
    "                        model_result(model,MODEL,ratio,testyear,[training_x_dynamic,training_x_static], [val_x_dynamic,val_x_static], \n",
    "                                             [test_x_dynamic,test_x_static], training_y, val_y, test_y, k,early_stop, reduce_lr)\n",
    "    if MODEL == ['DNN']:\n",
    "        # lstm\n",
    "        training_x_dynamic = np.array(X_train_dynamic).reshape(trainshape,912)\n",
    "        training_x_static = np.array(X_train_static).reshape(trainshape,19)\n",
    "        training_y = np.array(y_train).reshape(trainshape,1)\n",
    "\n",
    "        val_x_dynamic = np.array(X_val_dynamic).reshape(valshape,912)\n",
    "        val_x_static = np.array(X_val_static).reshape(valshape,19)\n",
    "        val_y = np.array(y_val).reshape(valshape,1)\n",
    "\n",
    "        test_x_dynamic = np.array(X_test_dynamic).reshape(testshape,912)\n",
    "        test_x_static = np.array(X_test_static).reshape(testshape,19)\n",
    "        test_y = np.array(y_test).reshape(testshape,1)\n",
    "        ratio = str(trainshape)+':'+str(valshape)+':'+str(testshape)\n",
    "\n",
    "        for i in UNIT:\n",
    "            for j in LR:\n",
    "                for k in BATCH_SIZE:\n",
    "                    print('UNIT=',i,\",LR=\",j,',BATCH_SIZE=',k)\n",
    "                    unit = i\n",
    "                    func_act = 'relu'\n",
    "                    inp1 = tf.keras.layers.Input(shape=(912))\n",
    "                    inp2 = tf.keras.layers.Input(shape=(19))\n",
    "                    inp = tf.keras.layers.Concatenate(axis=-1)([inp1, inp2])\n",
    "                    DENSE_0 = tf.keras.layers.Dense(units=i*128, activation=func_act)(inp)# ,kernel_regularizer=tf.keras.regularizers.l2(0.001)1028\n",
    "                    # DENSE_01 = tf.keras.layers.Dropout(0.2)(DENSE_0)\n",
    "                    DENSE_1 = tf.keras.layers.Dense(units=i*64, activation=func_act)(DENSE_0)#,kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "                    DENSE_2 = tf.keras.layers.Dense(units=i*32, activation=func_act)(DENSE_1)\n",
    "                    DENSE_3 = tf.keras.layers.Dense(units=i*16, activation=func_act)(DENSE_2)\n",
    "                    DENSE_4 = tf.keras.layers.Dense(units=i*8, activation=func_act)(DENSE_3)\n",
    "                    DENSE_5 = tf.keras.layers.Dense(units=i*4, activation=func_act)(DENSE_4)\n",
    "                    DENSE_6 = tf.keras.layers.Dense(units=i*2, activation=func_act)(DENSE_5)\n",
    "                    outp = tf.keras.layers.Dense(units=1, activation='linear')(DENSE_6)\n",
    "                    model = tf.keras.Model([inp1, inp2], outp)\n",
    "                    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=j), loss=tf.keras.losses.MeanSquaredError())# 'mean_absolute_error'\n",
    "                    early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1)\n",
    "                    reduce_lr = ReduceLROnPlateau(monitor='val_loss', min_delta=1e-3, factor=0.5, patience=5, verbose=1)\n",
    "#                     start = time.time()\n",
    "#                     history = model.fit([training_x_dynamic,training_x_static], training_y, shuffle=True, epochs=100, callbacks=[early_stop, reduce_lr], #\n",
    "#                                         validation_data=([val_x_dynamic,val_x_static], val_y), verbose=1, batch_size=k)# val_x_dynamic,val_x_static], val_y\n",
    "#                     end = time.time()\n",
    "                    if verbose == 0:\n",
    "                        raw,result = model_lineresult(model,MODEL,ratio,testyear,[training_x_dynamic,training_x_static], [val_x_dynamic,val_x_static], \n",
    "                                             [test_x_dynamic,test_x_static], training_y, val_y, test_y, k,early_stop, reduce_lr)\n",
    "                    else :\n",
    "                        model_result(model,MODEL,ratio,testyear,[training_x_dynamic,training_x_static], [val_x_dynamic,val_x_static], \n",
    "                                             [test_x_dynamic,test_x_static], training_y, val_y, test_y, k,early_stop, reduce_lr)\n",
    "    if Result:\n",
    "        # print(yield_mean, slope,raw,result)\n",
    "        # get the yield and pred\n",
    "        pred = pd.DataFrame([])\n",
    "#         result = pd.concat([data.loc[test_y.index,['sta_con', 'yield','irrigation','Planted_pdsi','Emergrd_pdsi','Blooming_pdsi','Setting Pods_pdsi',\n",
    "#                                                    'Dropping Leaves_pdsi','Harvested_pdsi']], pd.DataFrame(result,index = y_test.index,columns=['pred'])],axis=1)\n",
    "        result1= pd.DataFrame({'yield' : raw.flatten(),\n",
    "                                'pred' :  result.flatten()},\n",
    "                                columns=['yield','pred'])\n",
    "        print(slope,testyear[0],yield_mean,b)\n",
    "        print(r2_score(raw,result))\n",
    "        result1.loc[:,'yield'] =result1.loc[:,'yield']+ slope * testyear[0] - yield_mean + b\n",
    "        result1.loc[:,'pred'] =result1.loc[:,'pred']+ slope * testyear[0] - yield_mean + b\n",
    "        result1.loc[:,['yield','pred']] = result1.loc[:,['yield','pred']]*62.719012*0.001\n",
    "        result1.loc[:,'residual'] =  result1.loc[:,'pred'] - result1.loc[:,'yield']\n",
    "        print(r2_score(result1.loc[:,'yield'],result1.loc[:,'pred']))\n",
    "        result1.to_csv(r\"C:\\Users\\lx\\Desktop\\美国产量估计\\结果\\0614\" + str(testyear[0]) + '_LSTM'+str(seeds) + '_预测结果test.csv',header=True)\n",
    "        print('Results have been saved！')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2004]\n",
      "UNIT= 16 ,LR= 0.001 ,BATCH_SIZE= 20\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 00071: early stopping\n",
      "2022-06-16 [2004] 10358:1829:852 ['LSTM'] 【Train_time: 278.08, Validation_RMSE: 3.76, Validation_R2: 0.84, Test_RMSE: 6.76, Test_R2: 0.60】\n",
      "[0.48696257] 2004 45.21594321818331 [-935.12739114]\n",
      "0.596125098716502\n",
      "0.5961250987165001\n",
      "Results have been saved！\n",
      "[2005]\n",
      "UNIT= 16 ,LR= 0.001 ,BATCH_SIZE= 20\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 00099: early stopping\n",
      "2022-06-16 [2005] 10392:1835:812 ['LSTM'] 【Train_time: 399.31, Validation_RMSE: 3.65, Validation_R2: 0.86, Test_RMSE: 6.20, Test_R2: 0.51】\n",
      "[0.485661] 2005 45.07431095117363 [-932.55379031]\n",
      "0.5128674398375952\n",
      "0.5128674398375932\n",
      "Results have been saved！\n",
      "[2010]\n",
      "UNIT= 16 ,LR= 0.001 ,BATCH_SIZE= 20\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 00135: early stopping\n",
      "2022-06-16 [2010] 10498:1853:688 ['LSTM'] 【Train_time: 546.87, Validation_RMSE: 3.72, Validation_R2: 0.86, Test_RMSE: 4.24, Test_R2: 0.71】\n",
      "[0.48485819] 2010 44.9131163468545 [-930.94416554]\n",
      "0.7148197319301206\n",
      "0.7148197319301208\n",
      "Results have been saved！\n"
     ]
    }
   ],
   "source": [
    "for i in [ 2004, 2005, 2010]:#range(2003,2021,1):[2003,2013]:\n",
    "    tune_param(testyear = [i],MODEL=['LSTM'],verbose=0,shape=False,UNIT=[16],LR = [0.001],BATCH_SIZE=[20],Result=True,seeds=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2012]\n",
      "UNIT= 16 ,LR= 0.001 ,BATCH_SIZE= 20\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 00080: early stopping\n",
      "2022-06-16 [2012] 10486:1851:702 ['LSTM'] 【Train_time: 326.49, Validation_RMSE: 3.82, Validation_R2: 0.83, Test_RMSE: 7.53, Test_R2: 0.48】\n",
      "[0.5033154] 2012 45.31321228823863 [-967.65223448]\n",
      "0.48498737967490335\n",
      "0.4849873796749107\n",
      "Results have been saved！\n"
     ]
    }
   ],
   "source": [
    "for i in [2012]:#range(2003,2021,1):[2003,2013]:\n",
    "    tune_param(testyear = [i],MODEL=['LSTM'],verbose=0,shape=False,UNIT=[16],LR = [0.001],BATCH_SIZE=[20],Result=True,seeds=99)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2008]\n",
      "UNIT= 16 ,LR= 0.001 ,BATCH_SIZE= 20\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 00096: early stopping\n",
      "2022-06-16 [2008] 10491:1852:696 ['LSTM'] 【Train_time: 388.49, Validation_RMSE: 3.77, Validation_R2: 0.84, Test_RMSE: 4.84, Test_R2: 0.60】\n",
      "[0.49228977] 2008 45.23996597261605 [-945.69792063]\n",
      "0.6047100620195154\n",
      "0.6047100620194948\n",
      "Results have been saved！\n"
     ]
    }
   ],
   "source": [
    "for i in [2008]:#range(2003,2021,1):[2003,2013]:\n",
    "    tune_param(testyear = [i],MODEL=['LSTM'],verbose=0,shape=False,UNIT=[16],LR = [0.001],BATCH_SIZE=[20],Result=True,seeds=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_358\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_367 (InputLayer)          [(None, 24, 38)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_119 (LSTM)                 (None, 16)           3520        input_367[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_368 (InputLayer)          [(None, 19)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_177 (Concatenate)   (None, 35)           0           lstm_119[0][0]                   \n",
      "                                                                 input_368[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1018 (Dense)              (None, 128)          4608        concatenate_177[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1019 (Dense)              (None, 1)            129         dense_1018[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,257\n",
      "Trainable params: 8,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # 保存训练好的模型\n",
    "# model.save(r'C:\\Users\\lx\\Desktop\\美国产量估计\\已经训练好的模型\\LSTM')\n",
    "# 读取模型\n",
    "model = tf.keras.models.load_model(r'C:\\Users\\lx\\Desktop\\美国产量估计\\已经训练好的模型\\LSTM')#\\DNN\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'end' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_182132/269633264.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 训练时间\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'训练时间：'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# 训练评估\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_x_dynamic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_x_static\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'end' is not defined"
     ]
    }
   ],
   "source": [
    "# 训练时间\n",
    "print('训练时间：',end-start)\n",
    "# 训练评估\n",
    "y_train_pred = model.predict([training_x_dynamic,training_x_static])\n",
    "print(train_metrics(training_y, y_train_pred))\n",
    "# 验证评估\n",
    "y_val_pred = model.predict([val_x_dynamic,val_x_static])\n",
    "print(test_metrics(val_y, y_val_pred))\n",
    "# 测试评估1\n",
    "y_test_pred = model.predict([test_x_dynamic,test_x_static])\n",
    "print(test_metrics(test_y, y_test_pred))\n",
    "\n",
    "print('检查是否过拟合：')\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_x_dynamic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_182132/1822331656.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_yield\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_x_dynamic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_x_static\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mPRED_YIELD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_yield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'yield'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'linear regression '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# linear regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1840\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_x_dynamic' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_yield = model.predict([test_x_dynamic,test_x_static])\n",
    "PRED_YIELD = pd.DataFrame(pred_yield, columns=['yield'])\n",
    "print('linear regression ')\n",
    "# linear regression \n",
    "y_test = np.array(test_y).reshape(1840 ,1)\n",
    "P_y = np.array(PRED_YIELD['yield']).reshape(1840 ,1)\n",
    "linear_m=linear_model.LinearRegression()\n",
    "linear_m.fit(y_test,P_y)\n",
    "\n",
    "print('plotting\\n')\n",
    "#plot result\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(y_test,P_y,c='k')\n",
    "plt.plot(y_test,y_test,'r-',label='1:1')\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.xlabel('Truth')\n",
    "plt.ylabel('Predicted')\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "X_range=np.arange(xmin, xmax)\n",
    "Y_range=linear_m.predict(X_range.reshape(len(X_range),1))\n",
    "plt.plot(X_range,Y_range,'b--',label='y=%.2fx'%(linear_m.coef_[0]))\n",
    "plt.text(25,55,'$\\ R^2$=%.2f'%r2_score(y_test, P_y))\n",
    "plt.legend()\n",
    "#plt.savefig(u'/content/drive/MyDrive/csv_data/10_29只用lswi_2层_动态+静态特征'+'.jpg',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyc_py3.5",
   "language": "python",
   "name": "py3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
